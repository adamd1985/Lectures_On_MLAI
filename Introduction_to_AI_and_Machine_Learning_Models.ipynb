{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamd1985/lectutures_on_AI/blob/main/Introduction_to_AI_and_Machine_Learning_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b72fac",
      "metadata": {
        "id": "81b72fac"
      },
      "source": [
        "#  Introduction to AI and Machine Learning - Models\n",
        "\n",
        "All datasets used within our modules are available from: https://scikit-learn.org/stable/datasets/toy_dataset.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73723e96",
      "metadata": {
        "id": "73723e96"
      },
      "source": [
        "## Supervised Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb8872fc",
      "metadata": {
        "id": "cb8872fc"
      },
      "source": [
        "### Linear Regression\n",
        "\n",
        "\n",
        "Weâ€™re going to break down a simple code example, understand the imports, and dive into the purpose of each line.\n",
        "\n",
        "Let's go through the code step-by-step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These libraries will be your main tools for the lessons:\n",
        "\n",
        "- NumPy is a library for handling numerical data in Python.\n",
        "- matplotlib: Matplotlib is a plotting library, and  provides a simple interface for creating charts.\n",
        "- sklearn: Scikit-Learn is a powerful machine learning library in Python that contains all algorithms we will need.\n"
      ],
      "metadata": {
        "id": "OksLn4NIA87c"
      },
      "id": "OksLn4NIA87c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713f6cbc",
      "metadata": {
        "id": "713f6cbc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fetch a public dataset, in this case the california housing data with these attributes:\n",
        "- **MedInc**: median income in block group\n",
        "- **HouseAge**: median house age in block group\n",
        "- **AveRooms**: average number of rooms per household\n",
        "- **AveBedrms**: average number of bedrooms per household\n",
        "- **Population**: block group population\n",
        "- **AveOccup**: average number of household members\n",
        "- **Latitude**: block group latitude\n",
        "- **Longitude**: block group longitude\n",
        "- **MedHouseVal**: the median of the house value for each district and our target value."
      ],
      "metadata": {
        "id": "9fjhJKyZBArE"
      },
      "id": "9fjhJKyZBArE"
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing = fetch_california_housing(as_frame=True)\n",
        "df = cali_housing.frame\n",
        "df.sample(3)"
      ],
      "metadata": {
        "id": "8_bIhqMTEU3x"
      },
      "id": "8_bIhqMTEU3x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to predict the **MedHouseVal**.\n",
        "\n",
        "To train the linear regression model, we first have to create a feature vector off all other attributes, except `MedHouseVal`. This is called `X`.\n",
        "The vector with our target value is called `Y`.\n",
        "\n",
        "`train_test_split` is a utility function commonly used to split the vectors into a train and test set. Here we leave 20% of the data to be unseen by the model, to be evaluated later.\n"
      ],
      "metadata": {
        "id": "DGXyQRQyHcmj"
      },
      "id": "DGXyQRQyHcmj"
    },
    {
      "cell_type": "code",
      "source": [
        "X = cali_housing.data\n",
        "y = cali_housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "QSTEcfuxBG1m"
      },
      "id": "QSTEcfuxBG1m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we train our model using the train test splits created above."
      ],
      "metadata": {
        "id": "6pGGtplBI36m"
      },
      "id": "6pGGtplBI36m"
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "coefficients = pd.DataFrame({\n",
        "    \"Feature\": X_train.columns,\n",
        "    \"Coefficient\": model.coef_\n",
        "})\n",
        "\n",
        "print(\"Coefficients (Beta1, Beta2,...BetaN):\")\n",
        "print(coefficients)\n",
        "print(\"Intercept (Beta0):\", model.intercept_)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-squared (R^2): {r2}\")"
      ],
      "metadata": {
        "id": "IYzCGaw8Ee0P"
      },
      "id": "IYzCGaw8Ee0P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test on some known samples from the dataset:\n",
        "\n",
        "| MedInc | HouseAge | AveRooms  | AveBedrms  | Population | AveOccup | Latitude | Longitude | MedHouseVal |\n",
        "|--------|----------|-----------|------------|------------|----------|----------|-----------|-------------|\n",
        "| 8.3252 | 41.0     | 6.984127  | 1.023810   | 322.0      | 2.555556 | 37.88    | -122.23   | 4.526       |\n",
        "| 8.3014 | 21.0     | 6.238137  | 0.971880   | 2401.0     | 2.109842 | 37.86    | -122.22   | 3.585       |\n",
        "| 7.2574 | 52.0     | 8.288136  | 1.073446   | 496.0      | 2.802260 | 37.85    | -122.24   | 3.521       |\n",
        "\n",
        "From our lesson, we also provide the **MSE** using the API `mean_squared_error` from Scikit-Learn, we use this to validate the performance of the model."
      ],
      "metadata": {
        "id": "YjHj0ylOuyzk"
      },
      "id": "YjHj0ylOuyzk"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = {\n",
        "    \"MedInc\": [8.3252, 8.3014, 7.2574],\n",
        "    \"HouseAge\": [41.0, 21.0, 52.0],\n",
        "    \"AveRooms\": [6.984127, 6.238137, 8.288136],\n",
        "    \"AveBedrms\": [1.023810, 0.971880, 1.073446],\n",
        "    \"Population\": [322.0, 2401.0, 496.0],\n",
        "    \"AveOccup\": [2.555556, 2.109842, 2.802260],\n",
        "    \"Latitude\": [37.88, 37.86, 37.85],\n",
        "    \"Longitude\": [-122.23, -122.22, -122.24],\n",
        "    \"MedHouseVal\": [4.526, 3.585, 3.521]\n",
        "}\n",
        "\n",
        "sample_df = pd.DataFrame(sample_data)\n",
        "sample_y = sample_df[\"MedHouseVal\"]\n",
        "sample_X = sample_df.drop(columns=[\"MedHouseVal\"])\n",
        "y_sample_pred = model.predict(sample_X)\n",
        "\n",
        "\n",
        "sample_df[\"Predicted (Y)\"] = y_sample_pred\n",
        "sample_df[\"Error (E)\"] = sample_y - y_sample_pred\n",
        "\n",
        "mse = mean_squared_error(sample_y, y_sample_pred)\n",
        "print(f\"\\nMean Squared Error: {mse}\")\n",
        "print(\"Predicted vs Actual Values:\")\n",
        "sample_df"
      ],
      "metadata": {
        "id": "5Vomlvryu5rF"
      },
      "id": "5Vomlvryu5rF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we visualize the result of our model on the test data, which it has never seen. In general the predictions are close to the actual data points, with the exception of extreme values.\n",
        "\n",
        "To avoid issues caused by extreme values, we do feature engineering and data cleaning. More of that in the lessons within this module.\n"
      ],
      "metadata": {
        "id": "YaqCMX3DJR8W"
      },
      "id": "YaqCMX3DJR8W"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, label=\"Predicted vs Actual\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2, label=\"Perfect Prediction\")\n",
        "\n",
        "plt.title('Predicted VS Actual Median House Values', fontsize=16)\n",
        "plt.xlabel('Actual Median House Values', fontsize=14)\n",
        "plt.ylabel('Predicted Median House Values', fontsize=14)\n",
        "plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "plt.legend(fontsize=12, loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "62PgNG7dEKHo"
      },
      "id": "62PgNG7dEKHo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3d76968a",
      "metadata": {
        "id": "3d76968a"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "In this example, we will use logistic regression and its error functions. The imports are almost the same as the linear regression section, with these additions:\n",
        "- **linear_model**: To inclue the logistic regression model\n",
        "- **metrics**: For classification metrics\n",
        "- **inspection**: For boundary plotting APIs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "Snb_hbCQ728L"
      },
      "id": "Snb_hbCQ728L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The breast cancer dataset is one commonly used for binary  classification, and will be a good example for logistic regression. The dataset has 30 features, though we list the most used ones here:\n",
        "\n",
        "- **mean radius**: Mean of distances from center to points on the perimeter\n",
        "- **mean texture**: Standard deviation of gray-scale values\n",
        "- **mean perimeter**: Mean size of the core tumor perimeter\n",
        "- **mean area**: Mean size of the core tumor area\n",
        "- **mean smoothness**: Mean of local variation in radius lengths                |\n",
        "- Target: **0**: Malignant (Cancerous) or **1**: Benign (Non-Cancerous)\n"
      ],
      "metadata": {
        "id": "AytzOjac-tTQ"
      },
      "id": "AytzOjac-tTQ"
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer = load_breast_cancer(as_frame=True)\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "breast_cancer.frame.sample(3)"
      ],
      "metadata": {
        "id": "W0aHeZ_573_g"
      },
      "id": "W0aHeZ_573_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as linear regression, collect the features in `X`, the target in `y` and split into training and testing datasets.\n",
        "Note for this particular model, we choose to scale the data. More on this feature engineering later."
      ],
      "metadata": {
        "id": "241pu3t-A8Le"
      },
      "id": "241pu3t-A8Le"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "18jSFc7RA7E2"
      },
      "id": "18jSFc7RA7E2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train a logistic regression model here. Unlike linear regression, we evaluate the model using classification-specific metrics:\n",
        "\n",
        "- **Accuracy**: The percentage of correctly predicted instances out of the total instances. For this dataset, accuracy is quite high, reflecting the model's effectiveness.\n",
        "- **Precision**: The proportion of correctly predicted positive instances (**True Positives, TP**) out of all predicted positive instances, while **True Negatives (TN)** refer to correctly predicted negative cases.\n",
        "- **Recall**: The proportion of correctly predicted positive instances (**TP**) out of all actual positive instances:\n",
        "  $$\n",
        "  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
        "  $$\n",
        "- **F1-Score**: The harmonic mean of precision and recall, providing a balanced measure.\n"
      ],
      "metadata": {
        "id": "sYHSzABsMa-G"
      },
      "id": "sYHSzABsMa-G"
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "coefficients = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Coefficient\": model.coef_[0]\n",
        "})\n",
        "\n",
        "print(\"Coefficients (Beta1, Beta2,...BetaN):\")\n",
        "print(coefficients)\n",
        "print(\"Intercept (Beta0):\", model.intercept_[0])\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "2rPZfuFuCgO6"
      },
      "id": "2rPZfuFuCgO6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the **confusion matrix**, we analyze the following:\n",
        "- **True Positives (TP)**: Correctly predicted positive cases.\n",
        "- **True Negatives (TN)**: Correctly predicted negative cases.\n",
        "- **False Positives (FP)**: Incorrectly predicted positive cases (actual negative).\n",
        "- **False Negatives (FN)**: Incorrectly predicted negative cases (actual positive)."
      ],
      "metadata": {
        "id": "TZ3z3N6aN1_7"
      },
      "id": "TZ3z3N6aN1_7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c6ab6b8",
      "metadata": {
        "id": "6c6ab6b8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    model, X_test_scaled, y_test, display_labels=breast_cancer.target_names, cmap=\"Blues\", values_format=\"d\"\n",
        ")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression predicts probabilities for binary outcomes (0 or 1), therefore the threshold at which the model predicts class 1 versus class 0. We can visualize this using the following [api](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html) from Scikit Learn.\n",
        "\n",
        "Note that for linear models like the logistic, you can also plot the boundaries using their function, in our case we can derive it from the base logistic function: $X_2 = -\\frac{(w_1 \\cdot X_1 + b)}{w_2}$ which represents the decision boundary where the model predicts equal probabilities for both classes (0.5).\n",
        "\n",
        "Don't worry about the pipeline, we use it to include data scaling in the model's operations, but keep the data raw for visualization."
      ],
      "metadata": {
        "id": "D7yS11LSN5tr"
      },
      "id": "D7yS11LSN5tr"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reg_decision_boundary(\n",
        "    X, y, ax, feature_names, class_names\n",
        "):\n",
        "  reg_clf = make_pipeline(StandardScaler(), LogisticRegression()).fit(X, y)\n",
        "  common_params = {\"estimator\": reg_clf, \"X\": X, \"ax\": ax}\n",
        "\n",
        "  # Shows the regions where the classifier assigns each class.\n",
        "  DecisionBoundaryDisplay.from_estimator(\n",
        "      **common_params,\n",
        "      response_method=\"predict\",\n",
        "      plot_method=\"contourf\",\n",
        "      cmap=plt.cm.Paired,\n",
        "      alpha=0.3,\n",
        "      eps=0.5,\n",
        "  )\n",
        "  scatter = ax.scatter(\n",
        "      X.iloc[:, 0],\n",
        "      X.iloc[:, 1],\n",
        "      c=y,\n",
        "      cmap=plt.cm.Paired,\n",
        "      edgecolors=\"k\",\n",
        "      s=50,\n",
        "  )\n",
        "  legend_labels = [class_names[int(label)] for label in np.unique(y)]\n",
        "  ax.legend(\n",
        "      scatter.legend_elements()[0],\n",
        "      legend_labels,\n",
        "      loc=\"upper right\",\n",
        "      title=\"Classes\",\n",
        "  )\n",
        "\n",
        "  ax.set_title(f\"Decision boundaries of Logistic Regression\\n({feature_names[0]} vs {feature_names[1]})\")\n",
        "\n",
        "\n",
        "feature_names = [\"worst concave points\", \"worst radius\", \"mean concave points\"]\n",
        "feature_pairs = [\n",
        "    (0, 1, [\"worst concave points\", \"worst radius\"]),\n",
        "    (0, 2, [\"worst concave points\", \"mean concave points\"]),\n",
        "    (1, 2, [\"worst radius\", \"mean concave points\"]),\n",
        "]\n",
        "\n",
        "target_names = breast_cancer.target_names\n",
        "fig, axes = plt.subplots(1, 3, figsize=(21, 7), tight_layout=True)\n",
        "for ax, (idx1, idx2, names) in zip(axes, feature_pairs):\n",
        "    X_pair = X_test.iloc[:, [idx1, idx2]]\n",
        "    y_pair = y_test\n",
        "    plot_reg_decision_boundary(\n",
        "        X_pair, y_pair, ax, names, target_names\n",
        "    )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AqysZdjR7stE"
      },
      "id": "AqysZdjR7stE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the size of the features, it is not feasible to construct a new row, therefore we will sample some test rows and validate their class."
      ],
      "metadata": {
        "id": "7MiMopBp3Afz"
      },
      "id": "7MiMopBp3Afz"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_indices = np.random.choice(len(X_test), 3, replace=False)\n",
        "X_sample = X_test_scaled[sample_indices]\n",
        "y_actual = y_test.iloc[sample_indices]\n",
        "y_pred_sample = model.predict(X_sample)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    print(f\"Actual Class: {y_actual.iloc[i]}, Predicted Class: {y_pred_sample[i]}\")\n",
        "X_test.iloc[sample_indices]"
      ],
      "metadata": {
        "id": "F0WCQsu63x3g"
      },
      "id": "F0WCQsu63x3g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9cce04c4",
      "metadata": {
        "id": "9cce04c4"
      },
      "source": [
        "### Decision Trees\n",
        "\n",
        "For a decision tree, we import the APIs from Scikit-Learn, note how the tree has both a classifier and a regressor."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree"
      ],
      "metadata": {
        "id": "hqCj-KHp9oty"
      },
      "id": "hqCj-KHp9oty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load again the breast cancer dataset. Note that this time, we don't need to scale it as decision trees can work with raw data."
      ],
      "metadata": {
        "id": "mznd3cK-91EO"
      },
      "id": "mznd3cK-91EO"
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer = load_breast_cancer(as_frame=True)\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "breast_cancer.frame.head(3)"
      ],
      "metadata": {
        "id": "AiLdgMbc95bZ"
      },
      "id": "AiLdgMbc95bZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the classifier, and compare with  the logistic regression metrics of:\n",
        "\n",
        "```bash\n",
        "Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.98      0.95      0.96        43\n",
        "           1       0.97      0.99      0.98        71\n",
        "\n",
        "    accuracy                           0.97       114\n",
        "   macro avg       0.97      0.97      0.97       114\n",
        "weighted avg       0.97      0.97      0.97       114\n",
        "```"
      ],
      "metadata": {
        "id": "Qak4eXu_LuAU"
      },
      "id": "Qak4eXu_LuAU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4026ad",
      "metadata": {
        "id": "2b4026ad"
      },
      "outputs": [],
      "source": [
        "tree_model = DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_leaf=3)\n",
        "tree_model.fit(X_train, y_train)\n",
        "y_pred = tree_model.predict(X_test)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we said, trees are the most interpretable of the ML algorithms, you can easilily follow a decision on any sample you choose. Not just that, but you can identify the most important data points for the decision, starting with the root node `mean concave points`"
      ],
      "metadata": {
        "id": "2Nenjvb1gAZn"
      },
      "id": "2Nenjvb1gAZn"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    tree_model,\n",
        "    feature_names=breast_cancer.feature_names,\n",
        "    class_names=breast_cancer.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=10\n",
        ")\n",
        "plt.title(\"Decision Tree Visualization\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jmFQGdJi-xdd"
      },
      "id": "jmFQGdJi-xdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algo provides an API to rank the most important feature here"
      ],
      "metadata": {
        "id": "3sR-D8KugZMS"
      },
      "id": "3sR-D8KugZMS"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": breast_cancer.feature_names,\n",
        "    \"Importance\": tree_model.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "id": "i8uXg60i-1VI"
      },
      "id": "i8uXg60i-1VI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it with a regresion on the **california housing dataset** we previously used in our linear regression problem."
      ],
      "metadata": {
        "id": "8IaWBETsEOwK"
      },
      "id": "8IaWBETsEOwK"
    },
    {
      "cell_type": "code",
      "source": [
        "cali_housing = fetch_california_housing(as_frame=True)\n",
        "X = cali_housing.data\n",
        "y = cali_housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "y4wBcLEAETMD"
      },
      "id": "y4wBcLEAETMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a tree regressor. Compare the R2 and MSE with the Linear Regressor which were:\n",
        "\n",
        "```bash\n",
        "Mean Squared Error (MSE): 0.5558915986952444\n",
        "R-squared (R^2): 0.5757877060324508\n",
        "```"
      ],
      "metadata": {
        "id": "WFxiB7QQLUVH"
      },
      "id": "WFxiB7QQLUVH"
    },
    {
      "cell_type": "code",
      "source": [
        "tree_regressor = DecisionTreeRegressor(random_state=42, max_depth=4, min_samples_leaf=6)\n",
        "tree_regressor.fit(X_train, y_train)\n",
        "y_pred = tree_regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"\\nMean Squared Error (MSE): {mse}\")\n",
        "print(f\"R-squared (R^2): {r2}\")"
      ],
      "metadata": {
        "id": "NSY55ZkbEYch"
      },
      "id": "NSY55ZkbEYch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bit trickier to interpret since we are working with a continuous dependant variable as a target"
      ],
      "metadata": {
        "id": "TZGq8dzfguAU"
      },
      "id": "TZGq8dzfguAU"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    tree_regressor,\n",
        "    feature_names=X.columns,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=10\n",
        ")\n",
        "plt.title(\"Decision Tree Visualization for California Housing Data\")\n",
        "plt.show()\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Importance\": tree_regressor.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "id": "NRKLSS7LEHYA"
      },
      "id": "NRKLSS7LEHYA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the clustering of the regression output, the target variable needs to fall in one of the leave nodes, and we have a finite amount of those, therefore it cannot be a continuous line like our result from the linear regression."
      ],
      "metadata": {
        "id": "Q2d4TEaMg2ti"
      },
      "id": "Q2d4TEaMg2ti"
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot to compare predictions vs actual values\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "plt.plot([0, 5], [0, 5], '--r', label=\"Perfect Prediction\")\n",
        "plt.xlabel(\"Actual Median House Value\")\n",
        "plt.ylabel(\"Predicted Median House Value\")\n",
        "plt.title(\"Predicted vs Actual House Values\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b1mlsP29g2Dr"
      },
      "id": "b1mlsP29g2Dr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "66089cfe",
      "metadata": {
        "id": "66089cfe"
      },
      "source": [
        "### SVM\n",
        "\n",
        "Let's import the SVM libraries for both a linear and nonlinear regression: `SVC` which is the support vector classifier, read about it [here](https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "5CJtxkhLiNNP"
      },
      "id": "5CJtxkhLiNNP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the breast cancer dataset, and split for training."
      ],
      "metadata": {
        "id": "ZxDv5O8xiP9o"
      },
      "id": "ZxDv5O8xiP9o"
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer = load_breast_cancer(as_frame=True)\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "breast_cancer.frame.sample(3)"
      ],
      "metadata": {
        "id": "ArPZ8kKliXQA"
      },
      "id": "ArPZ8kKliXQA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale it (more on data engineering in later lectures) and fit the SVM model, note the `linear` kernel used for this fit.\n",
        "\n",
        "You can compare this model's performance with the Logistic Regression's, which were:\n",
        "\n",
        "```bash\n",
        "Accuracy: 0.97\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.98      0.95      0.96        43\n",
        "           1       0.97      0.99      0.98        71\n",
        "\n",
        "    accuracy                           0.97       114\n",
        "   macro avg       0.97      0.97      0.97       114\n",
        "weighted avg       0.97      0.97      0.97       114\n",
        "```"
      ],
      "metadata": {
        "id": "rHpKAKnUig3G"
      },
      "id": "rHpKAKnUig3G"
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))"
      ],
      "metadata": {
        "id": "2Qjcr5Q-iqwN"
      },
      "id": "2Qjcr5Q-iqwN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever you are evaluating, always pass the engineered data, in this case the scaled data. Else you will get confusing results."
      ],
      "metadata": {
        "id": "WCKkWQ7u9w3h"
      },
      "id": "WCKkWQ7u9w3h"
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    svm_model, X_test_scaled, y_test, display_labels=breast_cancer.target_names, cmap=\"Blues\", values_format=\"d\"\n",
        ")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4oFf3Lly3F32"
      },
      "id": "4oFf3Lly3F32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as we did in the logistic, we can lot the decision boundaries for the SVM using a similar function and the Scikit Learn API."
      ],
      "metadata": {
        "id": "QfguAty82H2M"
      },
      "id": "QfguAty82H2M"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_svm_decision_boundary(kernel, X, y, ax, feature_names, class_names):\n",
        "    # We create a pipeline to show the unscaled data in the plots, but model the scaled data.\n",
        "    svc_clf = make_pipeline(StandardScaler(), SVC(kernel=kernel, C=1)).fit(X, y)\n",
        "    common_params = {\"estimator\": svc_clf, \"X\": X, \"ax\": ax}\n",
        "\n",
        "    # Plot decision regions and boundaries\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        **common_params,\n",
        "        response_method=\"predict\",\n",
        "        plot_method=\"pcolormesh\",\n",
        "        alpha=0.3,\n",
        "    )\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        **common_params,\n",
        "        response_method=\"decision_function\",\n",
        "        plot_method=\"contour\",\n",
        "        levels=[-1, 0, 1],\n",
        "        colors=[\"k\", \"k\", \"k\"],\n",
        "        linestyles=[\"--\", \"-\", \"--\"],\n",
        "    )\n",
        "\n",
        "    # Highlight support vectors\n",
        "    support_vectors = svc_clf.named_steps['standardscaler'].inverse_transform(svc_clf.named_steps['svc'].support_vectors_)\n",
        "    ax.scatter(\n",
        "        support_vectors[:, 0],\n",
        "        support_vectors[:, 1],\n",
        "        s=150,\n",
        "        facecolors=\"none\",\n",
        "        edgecolors=\"k\",\n",
        "        label=\"Support Vectors\",\n",
        "    )\n",
        "\n",
        "    # plot the data points\n",
        "    scatter = ax.scatter(\n",
        "        X.iloc[:, 0],\n",
        "        X.iloc[:, 1],\n",
        "        c=y,\n",
        "        cmap=plt.cm.Paired,\n",
        "        edgecolors=\"k\",\n",
        "        s=50,\n",
        "    )\n",
        "\n",
        "    legend_labels = [class_names[int(label)] for label in np.unique(y)]\n",
        "    ax.legend(\n",
        "        scatter.legend_elements()[0],\n",
        "        legend_labels,\n",
        "        loc=\"upper right\",\n",
        "        title=\"Classes\",\n",
        "    )\n",
        "\n",
        "    ax.set_title(f\"Decision boundaries of {kernel} kernel in SVC\\n({feature_names[0]} vs {feature_names[1]})\")\n",
        "\n",
        "feature_names = [\"worst concave points\", \"worst radius\", \"mean concave points\"]\n",
        "feature_pairs = [\n",
        "    (0, 1, [\"worst concave points\", \"worst radius\"]),\n",
        "    (0, 2, [\"worst concave points\", \"mean concave points\"]),\n",
        "    (1, 2, [\"worst radius\", \"mean concave points\"]),\n",
        "]\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(21, 7), tight_layout=True)\n",
        "for ax, (idx1, idx2, names) in zip(axes, feature_pairs):\n",
        "    X_pair = X_test.iloc[:, [idx1, idx2]]  # Extract selected feature pair\n",
        "    y_pair = y_test  # Labels remain the same\n",
        "    plot_svm_decision_boundary(\n",
        "        \"linear\", X_pair, y_pair, ax, names, target_names\n",
        "    )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JnC3S_8sYxY7"
      },
      "id": "JnC3S_8sYxY7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The strenght of SVM comes from its ability to use nonlinear boundaries through a kernel trick where it casts the data to a higher dimension plane. Using the same breast cancer dataset, we can see how it performs (you can compare it to the models above)."
      ],
      "metadata": {
        "id": "Ulv679mu2h2f"
      },
      "id": "Ulv679mu2h2f"
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))"
      ],
      "metadata": {
        "id": "XOkyFiRhY55w"
      },
      "id": "XOkyFiRhY55w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    svm_model, X_test, y_test, display_labels=breast_cancer.target_names, cmap=\"Blues\", values_format=\"d\"\n",
        ")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6eMERAq4Y8j7"
      },
      "id": "6eMERAq4Y8j7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For visualizing boundaries, note that now the boundaries are nonlinear."
      ],
      "metadata": {
        "id": "qlWkFVXXeEiL"
      },
      "id": "qlWkFVXXeEiL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de45aac",
      "metadata": {
        "id": "9de45aac"
      },
      "outputs": [],
      "source": [
        "feature_names = [\"worst concave points\", \"worst radius\", \"mean concave points\"]\n",
        "feature_pairs = [\n",
        "    (0, 1, [\"worst concave points\", \"worst radius\"]),\n",
        "    (0, 2, [\"worst concave points\", \"mean concave points\"]),\n",
        "    (1, 2, [\"worst radius\", \"mean concave points\"]),\n",
        "]\n",
        "\n",
        "target_names = breast_cancer.target_names\n",
        "fig, axes = plt.subplots(1, 3, figsize=(21, 7), tight_layout=True)\n",
        "for ax, (idx1, idx2, names) in zip(axes, feature_pairs):\n",
        "    X_pair = X_test.iloc[:, [idx1, idx2]]  # Extract the selected feature pair\n",
        "    y_pair = y_test  # Labels remain the same\n",
        "    plot_svm_decision_boundary(\n",
        "        \"rbf\", X_pair, y_pair, ax, names, target_names\n",
        "    )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fd17ef0",
      "metadata": {
        "id": "4fd17ef0"
      },
      "source": [
        "## Unsupervised Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e60d051b",
      "metadata": {
        "id": "e60d051b"
      },
      "source": [
        "### KMeans\n",
        "\n",
        "For Kmeans we will load the its [clustering APIs](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) and the [Iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris), a well known toy dataset used to teach ML. It contains measurements of 150 iris flowers from three species: Setosa, Versicolor, and Virginica. Each flower is described by four features:\n",
        "\n",
        "- Sepal Length (cm)\n",
        "- Sepal Width (cm)\n",
        "- Petal Length (cm)\n",
        "- Petal Width (cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data.values[:,:3] # We are taking the first 3 columns only!\n",
        "y = iris.target\n",
        "\n",
        "iris.frame.sample(3)"
      ],
      "metadata": {
        "id": "Wl9JnB0snjpA"
      },
      "id": "Wl9JnB0snjpA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fit a kmeans model for 3 clusters based on the target label.\n",
        "\n",
        "Note that because it  is unsupervised, there isn't a ground truth to compare against, therefore we don't do the usual train-test split. KMeans assigns arbitary cluster IDs (e.g., 0, 1, 2) that do not correspond to the actual class labels (e.g., `Setosa`, `Versicolor`, `Virginica`), for us to get an 'accuracy' we need the **mode** of the cluster - that will be the cluster's label, and then we compare against it."
      ],
      "metadata": {
        "id": "cg8moHhCnqj-"
      },
      "id": "cg8moHhCnqj-"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "def map_clusters_to_labels(y_true, y_pred):\n",
        "  \"Map any arbitary cluster IDs to the most common label within them using its `mode`.\"\n",
        "  labels = np.zeros_like(y_pred)\n",
        "  for cluster in np.unique(y_pred):\n",
        "      mask = y_pred == cluster\n",
        "      labels[mask] = mode(y_true[mask])[0]\n",
        "  return labels\n",
        "\n",
        "y_mapped = map_clusters_to_labels(y, y_kmeans)\n",
        "accuracy = accuracy_score(y, y_mapped)\n",
        "print(f\"Clustering accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "U7aQnbvqnuEj"
      },
      "id": "U7aQnbvqnuEj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we plot them on a 3D  graph since we are using 3 of the 4 features. We use mode again to find what cluster ID maps to what label."
      ],
      "metadata": {
        "id": "_Vb1_6gmr90G"
      },
      "id": "_Vb1_6gmr90G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e10a3d4",
      "metadata": {
        "id": "6e10a3d4"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "for cluster in range(3):\n",
        "    ax.scatter(\n",
        "        X[y_kmeans == cluster, 0],\n",
        "        X[y_kmeans == cluster, 1],\n",
        "        X[y_kmeans == cluster, 2],\n",
        "        label=f'{iris.target_names[mode(y[y_kmeans == cluster])[0]]}',\n",
        "        s=30,\n",
        "        alpha=0.6,\n",
        "        zorder=1\n",
        "    )\n",
        "\n",
        "centroids = kmeans.cluster_centers_\n",
        "ax.scatter(\n",
        "    centroids[:, 0],\n",
        "    centroids[:, 1],\n",
        "    centroids[:, 2],\n",
        "    c='red',\n",
        "    marker='X',\n",
        "    s=100,\n",
        "    label='Centroids',\n",
        "    alpha=1.0,\n",
        "    zorder=5\n",
        ")\n",
        "\n",
        "ax.set_box_aspect(None, zoom=0.90)\n",
        "ax.set_title('K-Means with Iris Dataset', fontsize=14)\n",
        "ax.set_xlabel('Sepal Length', fontsize=12)\n",
        "ax.set_ylabel('Sepal Width', fontsize=12)\n",
        "ax.set_zlabel('Petal Length', fontsize=12)\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbddc54",
      "metadata": {
        "id": "abbddc54"
      },
      "source": [
        "### DBScan"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the DBScan API"
      ],
      "metadata": {
        "id": "lPJKVnLLwWVF"
      },
      "id": "lPJKVnLLwWVF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN"
      ],
      "metadata": {
        "id": "3xWt7C8pwZMA"
      },
      "id": "3xWt7C8pwZMA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load again Iris with 3 out of 4 features to allow us to visualize the clusters."
      ],
      "metadata": {
        "id": "20MDiYYhwYkr"
      },
      "id": "20MDiYYhwYkr"
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data.values[:,:3] # We are taking the first 3 columns only!\n",
        "y = iris.target\n",
        "\n",
        "iris.frame.sample(3)"
      ],
      "metadata": {
        "id": "w1Jw5r8Wwe1t"
      },
      "id": "w1Jw5r8Wwe1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the algo"
      ],
      "metadata": {
        "id": "Na5bUOIGwlhn"
      },
      "id": "Na5bUOIGwlhn"
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.5, min_samples=6)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "y_mapped = map_clusters_to_labels(y, y_dbscan)\n",
        "accuracy = accuracy_score(y, y_mapped)\n",
        "print(f\"Clustering accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "-1a8I6oqwoEz"
      },
      "id": "-1a8I6oqwoEz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And plot, see the difference from K-Means?"
      ],
      "metadata": {
        "id": "aMJ6nO7-X7aU"
      },
      "id": "aMJ6nO7-X7aU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe81acf",
      "metadata": {
        "id": "ebe81acf"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "unique_labels = set(y_dbscan)\n",
        "\n",
        "for label in unique_labels:\n",
        "    class_member_mask = y_dbscan == label\n",
        "    core_points = X[class_member_mask]\n",
        "\n",
        "    if len(core_points) == 0:\n",
        "        continue\n",
        "\n",
        "    if label != -1:\n",
        "        ax.scatter(\n",
        "            core_points[:, 0],\n",
        "            core_points[:, 1],\n",
        "            core_points[:, 2],\n",
        "            s=50,\n",
        "            alpha=0.6,\n",
        "            zorder=1,\n",
        "            label=f'{iris.target_names[mode(y[class_member_mask])[0]]}'\n",
        "        )\n",
        "    else:\n",
        "        ax.scatter(\n",
        "            core_points[:, 0],\n",
        "            core_points[:, 1],\n",
        "            core_points[:, 2],\n",
        "            s=50,\n",
        "            alpha=0.6,\n",
        "            zorder=1,\n",
        "            label='Noise',\n",
        "            color='red',\n",
        "            marker='x'\n",
        "        )\n",
        "\n",
        "ax.set_box_aspect(None, zoom=0.90)\n",
        "ax.set_title('DBSCAN Clustering on Iris Dataset', fontsize=14)\n",
        "ax.set_xlabel('Sepal Length', fontsize=12)\n",
        "ax.set_ylabel('Sepal Width', fontsize=12)\n",
        "ax.set_zlabel('Petal Length', fontsize=12)\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754dc747",
      "metadata": {
        "id": "754dc747"
      },
      "source": [
        "### PCA\n",
        "\n",
        "let's load the PCA [API](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), and the Iris dataset again."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data.values # this time we loading all 4.\n",
        "y = iris.target\n",
        "\n",
        "iris.frame.sample(3)"
      ],
      "metadata": {
        "id": "vx-WbChHlw0k"
      },
      "id": "vx-WbChHlw0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use PCA, you always need to to standardize as it is sensitive to scale of the data. Here we want to reduce the dimensions from 4 to 2!"
      ],
      "metadata": {
        "id": "TomlZiEXmWFO"
      },
      "id": "TomlZiEXmWFO"
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(normalized_data)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f\"Explained variance of Principal Component 1: {explained_variance[0]*100:0.2f}%\")\n",
        "print(f\"Explained variance of Principal Component 2: {explained_variance[1]*100:0.2f}%\")"
      ],
      "metadata": {
        "id": "KrBBX1dvmUv9"
      },
      "id": "KrBBX1dvmUv9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "509502e9",
      "metadata": {
        "id": "509502e9"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot the PCA-reduced data\n",
        "plt.figure(figsize=(10, 7))\n",
        "for i, target_name in enumerate(iris.target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=target_name, alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.title(\"PCA of Iris Dataset\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we reduced by 1/2 the features would our AI be able to cluster them?\n",
        "\n",
        "K-Means previously had a Clustering accuracy of `88.67%`, let's see how it performs now."
      ],
      "metadata": {
        "id": "m7J9pZx90r3q"
      },
      "id": "m7J9pZx90r3q"
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_pca)\n",
        "y_mapped = map_clusters_to_labels(y, y_kmeans)\n",
        "\n",
        "accuracy = accuracy_score(y, y_mapped)\n",
        "print(f\"Clustering accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "for cluster in np.unique(y_mapped):\n",
        "    ax.scatter(\n",
        "        X_pca[y_mapped == cluster, 0],\n",
        "        X_pca[y_mapped == cluster, 1],\n",
        "        label=f'{iris.target_names[cluster]}',\n",
        "        s=30,\n",
        "        alpha=0.6,\n",
        "        zorder=1\n",
        "    )\n",
        "\n",
        "centroids = kmeans.cluster_centers_\n",
        "ax.scatter(\n",
        "    centroids[:, 0],\n",
        "    centroids[:, 1],\n",
        "    c='red',\n",
        "    marker='X',\n",
        "    s=100,\n",
        "    label='Centroids',\n",
        "    alpha=1.0,\n",
        "    zorder=5\n",
        ")\n",
        "ax.set_title('K-Means after PCA with Iris Dataset', fontsize=14)\n",
        "ax.set_xlabel('Principal Component 1', fontsize=12)\n",
        "ax.set_ylabel('Principal Component 2', fontsize=12)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Dx67BVr0rcG"
      },
      "id": "9Dx67BVr0rcG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "407a2bbd",
      "metadata": {
        "id": "407a2bbd"
      },
      "source": [
        "## Semi-Supervised Learning\n",
        "\n",
        "With SSL (Semi Supervised Learning), we can train classifiers on some labelled data.\n",
        "\n",
        "First we import the SSL [APIs](https://scikit-learn.org/1.5/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html#sklearn.semi_supervised.SelfTrainingClassifier), and we reduce the features to 2 using PCA.\n",
        "\n",
        "Then we remove some of the labels, creating an unlabelled dataset that the SSL AI needs to learning using LabelSpreading and SelfTraining."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "pca_components = [f\"Principal Component {i+1}\" for i in range(2)]\n",
        "\n",
        "# Let's mask 50% of the labels for demostation\n",
        "rng = np.random.RandomState(0)\n",
        "y_masked = np.copy(y)\n",
        "mask_indices = rng.rand(y.shape[0]) < 0.5\n",
        "y_masked[mask_indices] = -1\n"
      ],
      "metadata": {
        "id": "hi4C9--cL-yp"
      },
      "id": "hi4C9--cL-yp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train all models. Note the use of a baseline that will work directly with the fully labelled data. The use of a baseline is always recommended as it gives you a model to compare to.\n",
        "\n",
        "Also note how the Selftrained SVM has `probability=True` that is because the SelfTrainingClassifier works by pseudo-labeling the most confident predictions on the unlabeled data and uses the predicted probabilities of the underlying SVC to determine confidence."
      ],
      "metadata": {
        "id": "zhWJr1FeMXjz"
      },
      "id": "zhWJr1FeMXjz"
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Supervised SVM (Baseline)\": SVC(kernel=\"rbf\").fit(X_pca, y),\n",
        "    \"Label Spreading (50% unlabeled)\": LabelSpreading().fit(X_pca, y_masked),\n",
        "    \"Self-Training (50% unlabeled)\": SelfTrainingClassifier(SVC(kernel=\"rbf\", probability=True)).fit(X_pca, y_masked),\n",
        "}\n",
        "predictions = {name: model.predict(X_pca) for name, model in models.items()}\n",
        "accuracies = {name: f\"{accuracy_score(y, pred)*100.:0.2f}%\" for name, pred in predictions.items()}\n",
        "print(accuracies)"
      ],
      "metadata": {
        "id": "wL9fFY_dMXCs"
      },
      "id": "wL9fFY_dMXCs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the results."
      ],
      "metadata": {
        "id": "cfKOWecdO4sN"
      },
      "id": "cfKOWecdO4sN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c0f575",
      "metadata": {
        "id": "a1c0f575"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharex=True, sharey=True)\n",
        "colors = ['red', 'green', 'blue']\n",
        "markers = ['o', 's', '^']\n",
        "\n",
        "def plot_model(ax, model, title, accuracy, mask_indices=None, class_names = iris.target_names):\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        model, X_pca, response_method=\"predict\", ax=ax, alpha=0.5\n",
        "    )\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        ax.scatter(\n",
        "            X_pca[y == i, 0], X_pca[y == i, 1],\n",
        "            color=colors[i], marker=markers[i], edgecolor=\"k\", label=class_name\n",
        "        )\n",
        "\n",
        "    if mask_indices is not None:\n",
        "        ax.scatter(\n",
        "            X_pca[mask_indices, 0], X_pca[mask_indices, 1],\n",
        "            facecolors='none', edgecolors='black', s=100, label=\"Unlabeled\"\n",
        "        )\n",
        "\n",
        "    ax.set_title(f\"{title}\\nAccuracy: {accuracy}%\")\n",
        "    ax.set_xlabel(pca_components[0])\n",
        "    ax.set_ylabel(pca_components[1])\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "for ax, (name, model) in zip(axes, models.items()):\n",
        "    plot_model(ax, model, name, accuracies[name], mask_indices=mask_indices if \"SVM\" not in name else None)\n",
        "plt.suptitle(\n",
        "    \"SVM Baseline vs. Label Spreading vs. Self-Training\",\n",
        "    fontsize=16\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks.\n",
        "\n",
        "We will design a simple feed-forward neural network. We will use a framework called **pytorch** for this, read its documentation [here](https://pytorch.org/docs/stable/index.html).\n",
        "\n",
        "- **torch**: The base PyTorch module provides core functionalities such as tensor operations, which are the building blocks of neural networks.\n",
        "- **nn**: A module in PyTorch that provides classes and functions for building neural network layers.\n",
        "- **optim**: Provides optimizers for gradient-based optimization to update the model's weights to minimize the loss during training."
      ],
      "metadata": {
        "id": "zoHp5QHovxbp"
      },
      "id": "zoHp5QHovxbp"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "IzvWrLg2yGtf"
      },
      "id": "IzvWrLg2yGtf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load and prepare the breast cancer dataset again. Note that we are doing standardization, NNs are known to degrade quickly on unscaled data."
      ],
      "metadata": {
        "id": "P6Pr4ECtyI5u"
      },
      "id": "P6Pr4ECtyI5u"
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "RibboAcayZH4"
      },
      "id": "RibboAcayZH4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert the feature vectors to a structure called `tensor`. Tensors are multi-dimensional arrays (like NumPy arrays) used to store data and perform computations efficiently, with support for GPUs and automatic differentiation.\n",
        "\n",
        "They are **the core building blocks** for deep learning models, enabling operations like addition, multiplication, and gradient computation."
      ],
      "metadata": {
        "id": "nvN9rfw4yatu"
      },
      "id": "nvN9rfw4yatu"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
      ],
      "metadata": {
        "id": "j33IiraXyjnt"
      },
      "id": "j33IiraXyjnt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we build our simple NN.\n",
        "\n",
        "Since it is a classification problem we have to use a `sigmoid` function. But note how we default to `ReLU` in the deep layers. FC means fully connected."
      ],
      "metadata": {
        "id": "FJbaUfaGy74r"
      },
      "id": "FJbaUfaGy74r"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 16)  # First layer (16 neurons)\n",
        "        self.fc2 = nn.Linear(16, 8)           # Second layer (8 neurons)\n",
        "        self.fc3 = nn.Linear(8, 1)            # Output layer (1 neuron for binary classification)\n",
        "        self.sigmoid = nn.Sigmoid()           # Sigmoid for 0 or 1 output.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))           # ReLU for hidden layers\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "c5MYV-_VzrWj"
      },
      "id": "c5MYV-_VzrWj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create this NN.\n",
        "Note the loss function, we spoke a lot about MSE in our regressions, but given this is a binary classification, we have to use cross-enthropy.\n",
        "\n",
        "The ADAM optimizer is a widely used one for optimizing the gradient descent process."
      ],
      "metadata": {
        "id": "jyjxPLBkzshB"
      },
      "id": "jyjxPLBkzshB"
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]  # Number of features, 30 in the dataset.\n",
        "model = SimpleNN(input_size)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss, specific for classification.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Optimizes gradient descent."
      ],
      "metadata": {
        "id": "YkY_Rx1s0OIa"
      },
      "id": "YkY_Rx1s0OIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we run the training loop.\n",
        "\n",
        "Remember that we have to do the gradient descent for the model to update its weights and therefore **learn**.\n",
        "\n",
        "As we are on googlecolab, we can use their network inspection and visualization tools called **tensorboard**. Also we include `TQDM`, a well known progress bar library for when you have long running trainings."
      ],
      "metadata": {
        "id": "WoKcJhmm0Q5j"
      },
      "id": "WoKcJhmm0Q5j"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "writer = SummaryWriter(log_dir=\"runs/breast_cancer_nn\")\n",
        "writer.add_graph(model, X_train)\n",
        "\n",
        "activations = {}\n",
        "def activation_hook(name):\n",
        "    def hook(model, input, output):\n",
        "        activations[name] = output\n",
        "    return hook\n",
        "for name, layer in model.named_modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        layer.register_forward_hook(activation_hook(name))\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 50\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Metrics\n",
        "    with torch.no_grad():\n",
        "        y_pred_labels = (y_pred > 0.5).int()\n",
        "        train_accuracy = accuracy_score(y_train.numpy(), y_pred_labels.numpy())\n",
        "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
        "    writer.add_scalar(\"Accuracy/train\", train_accuracy, epoch)\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    writer.add_scalar(\"Learning Rate\", lr, epoch)\n",
        "    for name, param in model.named_parameters():\n",
        "        writer.add_histogram(f\"Weights/{name}\", param, epoch)\n",
        "        if param.grad is not None:\n",
        "            writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
        "    for name, activation in activations.items():\n",
        "        writer.add_histogram(f\"Activations/{name}\", activation, epoch)"
      ],
      "metadata": {
        "id": "QHRbJGMsxb5w"
      },
      "id": "QHRbJGMsxb5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run tensorboard in colab. In case its already runniing, we kill it and restart it.\n"
      ],
      "metadata": {
        "id": "OQbCGTtd6JJi"
      },
      "id": "OQbCGTtd6JJi"
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 $(ps aux | grep '[t]ensorboard' | awk '{print $2}')\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "TaqcKY-l3kGS"
      },
      "id": "TaqcKY-l3kGS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we evaluate the model, note 3 things happening in the code:\n",
        "1. `eval()` locks the layers of the model for evaluation - mostly effects dropouts.\n",
        "2. `no_grad()` api, since we are not learning anymore but using the model, we will save resources from gradient computations.\n",
        "3. `(y_pred_test > 0.5).int()`: because we want to convert probabilities to a label, remember the decision boundary from SVM?\n",
        "\n",
        "Compare it with the performance of our best model in logistic regression\n",
        "```bash\n",
        "Accuracy: 97.37%\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.98      0.95      0.96        43\n",
        "           1       0.97      0.99      0.98        71\n",
        "\n",
        "    accuracy                           0.97       114\n",
        "   macro avg       0.97      0.97      0.97       114\n",
        "weighted avg       0.97      0.97      0.97       114\n",
        "```"
      ],
      "metadata": {
        "id": "eZm_xFe50zRe"
      },
      "id": "eZm_xFe50zRe"
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_test = model(X_test)\n",
        "    y_pred_labels = (y_pred_test > 0.5).int()\n",
        "    accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_labels))\n",
        "\n",
        "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred_labels, display_labels=['No Cancer', 'Cancer'], cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "uxf48Nux0ypb"
      },
      "id": "uxf48Nux0ypb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Models\n",
        "\n",
        "For generative models, their architecture is usally quite large. For this introduction we will build a small VAE, which consists of an encoder that outputs parameters of a probability distribution (mean and log-variance) and a decoder that reconstructs the input from a sampled latent vector.\n",
        "\n",
        "Let's import the necessary libraries:\n",
        "- **DataLoader**: As we start working with large datasets, it won't be feasable to load everything in memory as we were doing with the scikit learn datasets. PyTorch comes equiped with loaders that deal with such datasets.\n",
        "- **datasets**: Same as the scikit one, has educational datasets.\n",
        "- **transforms**: Functions to apply transformations with the dataloaders, for example converting everything to a tensor."
      ],
      "metadata": {
        "id": "XCuME5h__X9E"
      },
      "id": "XCuME5h__X9E"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils"
      ],
      "metadata": {
        "id": "F0rwokjRhOhZ"
      },
      "id": "F0rwokjRhOhZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build the autoencoder here, which is made of the encoding process and the decoding process, in addition to the networks setups we used earlier."
      ],
      "metadata": {
        "id": "djAry1C1hTeW"
      },
      "id": "djAry1C1hTeW"
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Encoder layers\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2_mu = nn.Linear(128, 3)      # Mean of latent space\n",
        "        self.fc2_logvar = nn.Linear(128, 3)  # Log-variance of latent space\n",
        "\n",
        "        # Decoder layers\n",
        "        self.fc3 = nn.Linear(3, 128)\n",
        "        self.fc4 = nn.Linear(128, 28 * 28)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.relu(self.fc1(x))\n",
        "        mu = self.fc2_mu(h1)\n",
        "        logvar = self.fc2_logvar(h1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # The reparameterization trick.\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = torch.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 28 * 28))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "md0vT6YP_Zhr"
      },
      "id": "md0vT6YP_Zhr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function, as defined in the lesson using the Kullback-Leibler divergence."
      ],
      "metadata": {
        "id": "pPFWATRsh4cF"
      },
      "id": "pPFWATRsh4cF"
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # the reconstruciton loss, 1st term of the ELBO\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 28 * 28), reduction='sum')\n",
        "\n",
        "    # General KLD Equation with gaussian.\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Combined, is the VAE loss function\n",
        "    return BCE + KLD\n"
      ],
      "metadata": {
        "id": "OisP8KArh24f"
      },
      "id": "OisP8KArh24f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare the dataset. In python, images can be plotted as images, note how we use an iterator which loads from the disk rather than the memory for such large dataset.\n",
        "\n",
        "The chosen **MNIST dataset** (Modified National Institute of Standards and Technology) is a widely-used dataset in the machine learning and computer vision community:\n",
        "- **Dataset Size**: 70,000 images\n",
        "  - 60,000 images for training\n",
        "  - 10,000 images for testing\n",
        "- **Image Size**: 28x28 pixels (grayscale)\n",
        "- **Labels**: 10 (digits 0 through 9)\n",
        "- **Pixel Values**: Range from 0 (black) to 255 (white), normalized to [0, 1]."
      ],
      "metadata": {
        "id": "6HcH95DakuPO"
      },
      "id": "6HcH95DakuPO"
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
        "for i in range(2):\n",
        "    axs[i].imshow(images[i].squeeze(), cmap=\"gray\")\n",
        "    axs[i].set_title(f\"Label: {labels[i].item()}\")\n",
        "    axs[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vC1NEsdbkwpc"
      },
      "id": "vC1NEsdbkwpc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the VAE, this will take some time due to the dataset's size."
      ],
      "metadata": {
        "id": "LvbwiFr1kzuy"
      },
      "id": "LvbwiFr1kzuy"
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "EPOCHS = 5\n",
        "writer = SummaryWriter(log_dir=\"runs/vae_experiment\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{EPOCHS}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item() / len(data):.6f}')\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "    print(f'====> Epoch: {epoch+1} Average loss: {avg_train_loss:.4f}')\n",
        "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
        "\n",
        "    # WRite temp images to tensorboard.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_data, _ = next(iter(test_loader))\n",
        "        recon_batch, _, _ = model(test_data)\n",
        "        recon_batch = recon_batch.view(-1, 1, 28, 28)\n",
        "        comparison = torch.cat([test_data[:8], recon_batch[:8]])\n",
        "        img_grid = utils.make_grid(comparison, nrow=8)\n",
        "        writer.add_image('Reconstructed Images', img_grid, epoch)\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "rHlKsXCdk1cB"
      },
      "id": "rHlKsXCdk1cB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=runs/vae_experiment"
      ],
      "metadata": {
        "id": "zALOGHSLu6Oh"
      },
      "id": "zALOGHSLu6Oh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, we have encoded an image and the NN will try to reconstruct it, the image will be totally original though similar. This is the start of most GenAI models."
      ],
      "metadata": {
        "id": "a8UlSXzJmH3S"
      },
      "id": "a8UlSXzJmH3S"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image_grid(original_images, reconstructed_images, n=2):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i in range(n):\n",
        "        # Original images\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(original_images[i].view(28, 28).cpu().numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "        if i == n // 2:\n",
        "            ax.set_title('Original Images')\n",
        "\n",
        "        # Reconstructed images\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(reconstructed_images[i].view(28, 28).cpu().numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "        if i == n // 2:\n",
        "            ax.set_title('Reconstructed Images')\n",
        "    plt.show()\n",
        "\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "model.eval()\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataiter = iter(test_loader)\n",
        "images, _ = next(dataiter)\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed, _, _ = model(images)\n",
        "plot_image_grid(images, reconstructed, n=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "7oFHIC0amUDo"
      },
      "id": "7oFHIC0amUDo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclussion\n",
        "\n",
        "That was the last model. The next notebook will be about ML and Data engineering. A model is as good as the data you give it."
      ],
      "metadata": {
        "id": "7f10i9S0-udS"
      },
      "id": "7f10i9S0-udS"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "quant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
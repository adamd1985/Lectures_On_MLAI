{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamd1985/Lectures_On_MLAI/blob/main/4_6_DecisionTrees_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb1hgpq992UR"
      },
      "source": [
        "# Decision Trees\n",
        "\n",
        "In this notebook, we will explore how decision trees work, their fundamental concepts, and their application to real-world datasets. We will begin by understanding the structure of a decision tree, how it splits data based on features, and the mathematical criteria used to determine the best splits, such as Gini impurity and entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvP8QXcn92US"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Tree\n",
        "\n",
        "Trees are primarly used to classify data points."
      ],
      "metadata": {
        "id": "MmzvAaD0RpLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Impurity Metrics\n",
        "\n",
        "Decision trees rely on impurity measures to determine the best feature splits.\n",
        "\n",
        "The **Gini Impurity** measures how often a randomly chosen sample from the node would be incorrectly classified if labeled according to the majority class.\n",
        "\n",
        "$$\n",
        "G = 1 - \\sum_{k=1}^{K} p_{i,k}^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ p_{i,k} $ represents the proportion of data points belonging to class $ k $ at node $ i $.\n",
        "- $ K $ is the total number of classes in the dataset.\n",
        "\n",
        "A Gini Index of **0** means that the node is **pure**, meaning all instances belong to the same class. Higher values indicate greater impurity.\n",
        "\n",
        "\n",
        "Entropy, derived from **information theory**, measures the **uncertainty** of the dataset and is defined as:\n",
        "\n",
        "$\n",
        "H = -\\sum_{k=1}^{K} p_{i,k} \\log_2(p_{i,k})\n",
        "$\n",
        "\n",
        "where:\n",
        "- $ p_{i,k} $ is the probability of class $ k $ at node $ i $.\n",
        "- $ K $ is the total number of classes.\n",
        "\n",
        "Entropy is **0** when all samples belong to a single class, meaning the dataset is completely **pure**. The maximum entropy value occurs when all classes are equally probable, indicating **maximum disorder**.\n"
      ],
      "metadata": {
        "id": "ItP86p3oNFKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "def entropy(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    return -np.sum(probabilities * np.log2(probabilities + 1e-10)) # avoid div by 0!\n",
        "\n",
        "y1 = np.array([0, 0, 1, 1])  # 50% class 0, 50% class 1\n",
        "y2 = np.array([0, 0, 0, 1])  # 75% class 0, 25% class 1\n",
        "\n",
        "print(\"Gini Impurity and Entropy for y1:\")\n",
        "print(\"Gini:\", gini_impurity(y1))\n",
        "print(\"Entropy:\", entropy(y1))\n",
        "\n",
        "print(\"\\nGini Impurity and Entropy for y2:\")\n",
        "print(\"Gini:\", gini_impurity(y2))\n",
        "print(\"Entropy:\", entropy(y2))"
      ],
      "metadata": {
        "id": "FsNsK5vjIFpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information Gain\n",
        "\n",
        "Decision trees use **information gain** to determine which feature provides the most **useful split** at each step. Information gain is based on the reduction in **impurity** (measured by either **Gini Impurity** or **Entropy**) after splitting a dataset.\n",
        "\n",
        "\n",
        "$$\n",
        "IG = H_{\\text{parent}} - \\sum_{j} \\frac{N_j}{N_{\\text{total}}} H_j\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ H_{\\text{parent}} $ is the impurity (Entropy or Gini) of the **parent node**.\n",
        "- $ H_j $ is the impurity of the $ j $-th **child node**.\n",
        "- $ N_j $ is the number of samples in the $ j $-th child node.\n",
        "- $ N_{\\text{total}} $ is the total number of samples in the parent node."
      ],
      "metadata": {
        "id": "CE9OdXtgNzHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(y, left_y, right_y, cost_fn):\n",
        "    weight_left = len(left_y) / len(y)\n",
        "    weight_right = len(right_y) / len(y)\n",
        "\n",
        "    return cost_fn(y) - (weight_left * cost_fn(left_y) + weight_right * cost_fn(right_y))"
      ],
      "metadata": {
        "id": "9r_g9U86H2t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it on the lesson's toy salary dataset."
      ],
      "metadata": {
        "id": "uEyg59U_N-rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_df = pd.DataFrame({\n",
        "    'ID': [1, 2, 3, 4, 5],\n",
        "    'Age': [25, 32.5, 40, 37.5, 35],\n",
        "    'Salary': [50000, 60000, 80000, 90000, 70000],\n",
        "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male']\n",
        "})\n",
        "y = pd.Categorical(salary_df['Gender']).codes\n",
        "print(y)\n",
        "\n",
        "salary_df"
      ],
      "metadata": {
        "id": "hfVDf0eOKvEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Partitioning and Information Gain\n",
        "\n",
        "Decision trees use recursive partitioning, a method where the dataset is iteratively split into binary subsets until a stopping condition is met. The process follows a structured sequence to ensure that each decision maximizes the separation of different target classes or minimizes variance in regression.\n",
        "\n",
        "The classification and regression trees (CART) algorithm is a widely used implementation of recursive partitioning:\n",
        "\n",
        "1. Start at the root node, which contains the entire dataset.\n",
        "2. Identify the best feature to split on by evaluating all possible splits using a cost function such as Gini impurity for classification or variance reduction for regression.\n",
        "3. Perform binary splitting, dividing the dataset into two subsets at each step.\n",
        "4. Continue splitting nodes until a predefined stopping criterion is met, such as reaching a maximum depth or achieving pure leaf nodes.\n",
        "5. Apply pruning techniques to refine the tree structure and prevent overfitting by removing unnecessary branches.\n",
        "6. Finalize the model, ensuring it generalizes well to unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "fiWUlhbjOO5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gini_root = gini_impurity(y)\n",
        "\n",
        "print(\"Root: \", salary_df['ID'].values)\n",
        "print(\"Gini before split:\", gini_root)\n",
        "\n",
        "left_mask = salary_df['Age'] <= 35\n",
        "right_mask = ~left_mask\n",
        "\n",
        "left_y = y[left_mask]\n",
        "right_y = y[right_mask]\n",
        "\n",
        "info_gain = information_gain(y, left_y, right_y, gini_impurity)\n",
        "print(\"Information Gain for Age <= 35 split:\", info_gain)"
      ],
      "metadata": {
        "id": "s88izSzYLGiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gini_left = gini_impurity(left_y)\n",
        "gini_right = gini_impurity(right_y)\n",
        "print(\"Root: \", salary_df['ID'].values)\n",
        "print(\"Left: \", salary_df.loc[left_mask, 'ID'].values)\n",
        "print(\"Right: \", salary_df.loc[right_mask, 'ID'].values)\n",
        "\n",
        "print(\"Gini for left node (Age ≤ 35):\", gini_left)\n",
        "print(\"Gini for right node (Age > 35):\", gini_right)"
      ],
      "metadata": {
        "id": "O-7wqVVhLoSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "left_left_mask = salary_df.loc[left_mask, 'Salary'] <= 60000\n",
        "left_right_mask = ~left_left_mask\n",
        "\n",
        "left_left_y = left_y[left_left_mask]\n",
        "left_right_y = left_y[left_right_mask]\n",
        "\n",
        "info_gain_second_split = information_gain(left_y, left_left_y, left_right_y, gini_impurity)\n",
        "print(\"Information Gain for Salary ≤ 60,000 split in Left Node:\", info_gain_second_split)"
      ],
      "metadata": {
        "id": "HpnL0b7jLqba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gini_left_left = gini_impurity(left_left_y)\n",
        "gini_left_right = gini_impurity(left_right_y)\n",
        "print(\"Left IDs (≤ 60,000):\", salary_df.loc[left_mask].loc[left_left_mask, 'ID'].values if len(left_left_y) > 0 else \"Empty\")\n",
        "print(\"Right IDs (> 60,000):\", salary_df.loc[left_mask].loc[left_right_mask, 'ID'].values if len(left_right_y) > 0 else \"Empty\")\n",
        "\n",
        "print(\"Gini for left node (≤ 60,000):\", gini_left_left if gini_left_left is not None else \"N/A\")\n",
        "print(\"Gini for right node (> 60,000):\", gini_left_right if gini_left_right is not None else \"N/A\")"
      ],
      "metadata": {
        "id": "OZxKPhFtMLFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Classification Tree Build\n",
        "\n",
        "Let's build a full tree from ground up using these concepts"
      ],
      "metadata": {
        "id": "4ODNCR9OOsP6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEwmX3Xi92UY"
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, prediction=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.prediction = prediction\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.prediction is not None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, cost_function=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.cost_function = cost_function\n",
        "        self.root = None  # Root node from class Node\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        print(\"\\nStarting to build the Decision Tree...\\n\")\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        print(\"\\nFinished building the Decision Tree!\\n\")\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "        if len(unique_classes) == 1 or (self.max_depth and depth >= self.max_depth):\n",
        "            leaf_class = Counter(y).most_common(1)[0][0]\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Class = {leaf_class}\")\n",
        "            return Node(prediction=leaf_class)\n",
        "\n",
        "        best_feature, best_threshold, feature_scores_history = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            leaf_class = Counter(y).most_common(1)[0][0]\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Class = {leaf_class}\")\n",
        "            return Node(prediction=leaf_class)\n",
        "\n",
        "        print(f\"{'  ' * depth}Feature evaluation at depth {depth}:\")\n",
        "        for feat, (thresh, score) in feature_scores_history.items():\n",
        "            print(f\"{'  ' * depth}  Feature {feat} -> Best Threshold: {thresh:.4f}, Score: {score:.6f}\")\n",
        "\n",
        "        print(f\"{'  ' * depth}Depth {depth}: Selected Feature {best_feature} at Threshold {best_threshold}\")\n",
        "\n",
        "        left_indices = X[:, best_feature] <= best_threshold\n",
        "        right_indices = ~left_indices\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        best_gain = -1\n",
        "        best_feature, best_threshold = None, None\n",
        "        feature_scores_history = {}\n",
        "\n",
        "        for feature in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            best_feature_score = -1\n",
        "            best_feature_threshold = None\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_y = y[X[:, feature] <= threshold]\n",
        "                right_y = y[X[:, feature] > threshold]\n",
        "\n",
        "                if len(left_y) == 0 or len(right_y) == 0:\n",
        "                    continue\n",
        "\n",
        "                gain = information_gain(y, left_y, right_y, self.cost_function)\n",
        "                if gain > best_feature_score:\n",
        "                    best_feature_score = gain\n",
        "                    best_feature_threshold = threshold\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain, best_feature, best_threshold = gain, feature, threshold\n",
        "\n",
        "            feature_scores_history[feature] = (best_feature_threshold, best_feature_score)\n",
        "\n",
        "        return best_feature, best_threshold, feature_scores_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(sample, self.root) for sample in X])\n",
        "\n",
        "    def _traverse_tree(self, sample, node):\n",
        "        if node.is_leaf():\n",
        "            return node.prediction\n",
        "        if sample[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(sample, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(sample, node.right)\n"
      ],
      "metadata": {
        "id": "EpNuGopd3Kmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X, y = iris.data, iris.target\n",
        "iris.frame.sample(3)"
      ],
      "metadata": {
        "id": "j4LLvuA9UBB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test it."
      ],
      "metadata": {
        "id": "-BrmOPEBO76i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = iris.data.values, iris.target.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "tree = DecisionTree(max_depth=3, cost_function=gini_impurity)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "predictions = tree.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "9TnVJeFQ4Rvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrunedDecisionTree(DecisionTree):\n",
        "    def __init__(self, max_depth=None, cost_function=None, alpha=0.01):\n",
        "        super().__init__(max_depth, cost_function)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def _evaluate_accuracy(self, X_val, y_val):\n",
        "        predictions = self.predict(X_val)\n",
        "        return np.mean(predictions == y_val)\n",
        "\n",
        "\n",
        "    def prune(self, X_val, y_val):\n",
        "        print(\"\\nStarting pruning process...\\n\")\n",
        "        self._prune_tree(self.root, X_val, y_val, depth=0)\n",
        "        print(\"\\nPruning completed!\\n\")\n",
        "\n",
        "    def _prune_tree(self, node, X_val, y_val, depth):\n",
        "      if node.is_leaf():\n",
        "          return\n",
        "      self._prune_tree(node.left, X_val, y_val, depth + 1)\n",
        "      self._prune_tree(node.right, X_val, y_val, depth + 1)\n",
        "\n",
        "      if node.left.is_leaf() and node.right.is_leaf():\n",
        "          # Can it be pruned more?\n",
        "          pre_prune_accuracy = self._evaluate_accuracy(X_val, y_val)\n",
        "          left_node, right_node = node.left, node.right\n",
        "\n",
        "          node.prediction = Counter(y_val).most_common(1)[0][0]\n",
        "          node.left = None\n",
        "          node.right = None\n",
        "\n",
        "          post_prune_accuracy = self._evaluate_accuracy(X_val, y_val)\n",
        "\n",
        "          if post_prune_accuracy < pre_prune_accuracy - self.alpha:\n",
        "              # Revert pruning if we didn't improve!\n",
        "              node.prediction = None\n",
        "              node.left = left_node\n",
        "              node.right = right_node\n",
        "              print(f\"{'  ' * depth}Retained split at depth {depth}, feature {node.feature}, threshold {node.threshold}\")\n",
        "          else:\n",
        "              print(f\"{'  ' * depth}Pruned subtree at depth {depth}, feature {node.feature}, threshold {node.threshold}\")\n"
      ],
      "metadata": {
        "id": "nSwqHDMg6rH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tree = PrunedDecisionTree(max_depth=3, cost_function=gini_impurity, alpha=0.01)\n",
        "tree.fit(X_train, y_train)\n",
        "tree.prune(X_val, y_val)\n",
        "\n",
        "predictions = tree.predict(X_val)\n",
        "accuracy = np.mean(predictions == y_val)\n",
        "print(f\"Final Accuracy after pruning: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "bFRm9tJU6tWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check against the Scikit-learn evaluation."
      ],
      "metadata": {
        "id": "5570uhcLUyQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(max_depth=3, ccp_alpha=0.01, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "predictions = tree.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "print(f\"Final Accuracy after pruning: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "m_4G3xINTvTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Tree\n",
        "\n",
        "Trees can be used for regression, with minor alterations."
      ],
      "metadata": {
        "id": "8x9jZ1fIPf5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variance Reduction\n",
        "\n",
        "Regression trees use **variance reduction** to determine the best splits, ensuring that the target variable is more homogeneous within each resulting node. Unlike classification trees, which use impurity measures like Gini impurity or entropy, regression trees aim to minimize the **mean squared error (MSE)** within each subset.\n",
        "\n",
        "\n",
        "$\n",
        "VR = \\sigma^2_{\\text{parent}} - \\left( \\frac{N_{\\text{left}}}{N_{\\text{total}}} \\sigma^2_{\\text{left}} + \\frac{N_{\\text{right}}}{N_{\\text{total}}} \\sigma^2_{\\text{right}} \\right)\n",
        "$\n",
        "\n",
        "where:\n",
        "- $ \\sigma^2_{\\text{parent}} $ is the variance of the target variable before the split.\n",
        "- $ \\sigma^2_{\\text{left}} $ and $ \\sigma^2_{\\text{right}} $ are the variances of the left and right child nodes.\n",
        "- $ N_{\\text{left}} $, $ N_{\\text{right}} $, and $ N_{\\text{total}} $ represent the number of samples in the left, right, and parent nodes, respectively."
      ],
      "metadata": {
        "id": "EhoP3HtVPnHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variance_reduction(y, left_y, right_y):\n",
        "    weight_left = len(left_y) / len(y)\n",
        "    weight_right = len(right_y) / len(y)\n",
        "\n",
        "    return np.var(y) - (weight_left * np.var(left_y) + weight_right * np.var(right_y))"
      ],
      "metadata": {
        "id": "Tb3v8Zo2Po8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = salary_df[\"Salary\"].values\n",
        "\n",
        "variance_root = np.var(y)\n",
        "print(\"Root Node IDs: \", salary_df[\"ID\"].values)\n",
        "print(\"Variance before split:\", variance_root)\n",
        "\n",
        "\n",
        "left_mask = salary_df[\"Age\"] <= 35\n",
        "right_mask = ~left_mask\n",
        "\n",
        "left_y = y[left_mask]\n",
        "right_y = y[right_mask]\n",
        "\n",
        "\n",
        "info_gain = variance_reduction(y, left_y, right_y)\n",
        "print(\"Variance Reduction for Age <= 35 split:\", info_gain)\n",
        "\n",
        "print(\"\\nLeft Subset (Age <= 35):\", salary_df[left_mask][\"ID\"].values)\n",
        "print(\"Right Subset (Age > 35):\", salary_df[right_mask][\"ID\"].values)"
      ],
      "metadata": {
        "id": "Ustco2XRQFM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65da6502-9b03-478b-94ab-13765c91f3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Node IDs:  [1 2 3 4 5]\n",
            "Variance before split: 200000000.0\n",
            "Variance Reduction for Age <= 35 split: 150000000.0\n",
            "\n",
            "Left Subset (Age <= 35): [1 2 5]\n",
            "Right Subset (Age > 35): [3 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Regression Tree Build\n",
        "\n",
        "Let's build a full tree from ground up using these concepts"
      ],
      "metadata": {
        "id": "9DH_H3acQ7HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionTree:\n",
        "    def __init__(self, max_depth=5):\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        print(\"\\nStarting to build the Regression Tree...\\n\")\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        print(\"\\nFinished building the Regression Tree!\\n\")\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        num_samples, num_features = X.shape\n",
        "        if len(np.unique(y)) == 1 or (self.max_depth and depth >= self.max_depth):\n",
        "            leaf_value = np.mean(y)\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Value = {leaf_value:.4f}\")\n",
        "            return Node(prediction=leaf_value)\n",
        "\n",
        "        best_feature, best_threshold, best_score, feature_scores_history = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            leaf_value = np.mean(y)\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Value = {leaf_value:.4f}\")\n",
        "            return Node(prediction=leaf_value)\n",
        "\n",
        "        print(f\"{'  ' * depth}Feature evaluation at depth {depth}:\")\n",
        "        for feat, (thresh, score) in feature_scores_history.items():\n",
        "            print(f\"{'  ' * depth}  Feature {feat} -> Best Threshold: {thresh:.4f}, Variance Reduction: {score:.6f}\")\n",
        "\n",
        "        print(f\"{'  ' * depth}Depth {depth}: Selected Feature {best_feature} at Threshold {best_threshold}, Variance Reduction: {best_score:.6f}\")\n",
        "\n",
        "        left_indices = X[:, best_feature] <= best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        best_score = float(\"inf\")\n",
        "        best_feature, best_threshold = None, None\n",
        "        feature_scores_history = {}\n",
        "\n",
        "        for feature in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            best_feature_score = float(\"inf\")\n",
        "            best_feature_threshold = None\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_y = y[X[:, feature] <= threshold]\n",
        "                right_y = y[X[:, feature] > threshold]\n",
        "\n",
        "                if len(left_y) == 0 or len(right_y) == 0:\n",
        "                    continue\n",
        "\n",
        "                score = variance_reduction(y, left_y, right_y)\n",
        "                if score < best_feature_score:\n",
        "                    best_feature_score = score\n",
        "                    best_feature_threshold = threshold\n",
        "\n",
        "                if score < best_score:\n",
        "                    best_score, best_feature, best_threshold = score, feature, threshold\n",
        "\n",
        "            feature_scores_history[feature] = (best_feature_threshold, best_feature_score)\n",
        "\n",
        "        return best_feature, best_threshold, best_score, feature_scores_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the target values for a given dataset.\"\"\"\n",
        "        return np.array([self._traverse_tree(sample, self.root) for sample in X])\n",
        "\n",
        "    def _traverse_tree(self, sample, node):\n",
        "        \"\"\"Recursively traverses the tree to make predictions.\"\"\"\n",
        "        if node.is_leaf():\n",
        "            return node.prediction\n",
        "        if sample[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(sample, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(sample, node.right)\n"
      ],
      "metadata": {
        "id": "eWKGVUcK9JHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "cal_df = fetch_california_housing(as_frame=True)\n",
        "cal_df.frame.sample(3)"
      ],
      "metadata": {
        "id": "fWB3wWmMUTGN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "46bbfeda-7927-45b0-dc97-ca45f17c2630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "16773  4.2421      43.0  5.835526   0.997807      1384.0  3.035088     37.69   \n",
              "1138   3.1300       8.0  5.539062   1.019531      1163.0  2.271484     39.69   \n",
              "12081  4.7965       5.0  5.840160   1.033966      3258.0  3.254745     33.76   \n",
              "\n",
              "       Longitude  MedHouseVal  \n",
              "16773    -122.48        2.575  \n",
              "1138     -121.56        1.683  \n",
              "12081    -117.54        1.608  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-620c12c8-b2e5-4eec-9bfc-8fe10d889980\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedHouseVal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16773</th>\n",
              "      <td>4.2421</td>\n",
              "      <td>43.0</td>\n",
              "      <td>5.835526</td>\n",
              "      <td>0.997807</td>\n",
              "      <td>1384.0</td>\n",
              "      <td>3.035088</td>\n",
              "      <td>37.69</td>\n",
              "      <td>-122.48</td>\n",
              "      <td>2.575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>3.1300</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.539062</td>\n",
              "      <td>1.019531</td>\n",
              "      <td>1163.0</td>\n",
              "      <td>2.271484</td>\n",
              "      <td>39.69</td>\n",
              "      <td>-121.56</td>\n",
              "      <td>1.683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12081</th>\n",
              "      <td>4.7965</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.840160</td>\n",
              "      <td>1.033966</td>\n",
              "      <td>3258.0</td>\n",
              "      <td>3.254745</td>\n",
              "      <td>33.76</td>\n",
              "      <td>-117.54</td>\n",
              "      <td>1.608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-620c12c8-b2e5-4eec-9bfc-8fe10d889980')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-620c12c8-b2e5-4eec-9bfc-8fe10d889980 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-620c12c8-b2e5-4eec-9bfc-8fe10d889980');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7c2d9139-ee34-4b5d-9111-9f9ec2a92c9a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c2d9139-ee34-4b5d-9111-9f9ec2a92c9a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7c2d9139-ee34-4b5d-9111-9f9ec2a92c9a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"cal_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"MedInc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8486605151649275,\n        \"min\": 3.13,\n        \"max\": 4.7965,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4.2421,\n          3.13,\n          4.7965\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HouseAge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.1266025033211,\n        \"min\": 5.0,\n        \"max\": 43.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          43.0,\n          8.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveRooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17251660396991558,\n        \"min\": 5.5390625,\n        \"max\": 5.84015984015984,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5.8355263157894735,\n          5.5390625,\n          5.84015984015984\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveBedrms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018201555485605963,\n        \"min\": 0.9978070175438597,\n        \"max\": 1.033966033966034,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9978070175438597,\n          1.01953125,\n          1.033966033966034\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Population\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1151.067765164154,\n        \"min\": 1163.0,\n        \"max\": 3258.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1384.0,\n          1163.0,\n          3258.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AveOccup\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5160977714121507,\n        \"min\": 2.271484375,\n        \"max\": 3.254745254745255,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.0350877192982457,\n          2.271484375,\n          3.254745254745255\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0168913360168164,\n        \"min\": 33.76,\n        \"max\": 39.69,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          37.69,\n          39.69,\n          33.76\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Longitude\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.6271150209561296,\n        \"min\": -122.48,\n        \"max\": -117.54,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -122.48,\n          -121.56,\n          -117.54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MedHouseVal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5379556982999003,\n        \"min\": 1.608,\n        \"max\": 2.575,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.575,\n          1.683,\n          1.608\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = cal_df.data.values, cal_df.target.values\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, y_train = X[:200], y[:200]\n",
        "X_test, y_test = X[-20:], y[-20:]\n",
        "\n",
        "tree = RegressionTree(max_depth=5)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "predictions = tree.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f\"\\nMean Squared Error on Test Set: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "bedOeom59o8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f57a4af-a75a-45c0-9960-91bc0ec7788c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting to build the Regression Tree...\n",
            "\n",
            "Feature evaluation at depth 0:\n",
            "  Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.008262\n",
            "  Feature 1 -> Best Threshold: 1.6178, Variance Reduction: 0.000050\n",
            "  Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.005838\n",
            "  Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 0.000095\n",
            "  Feature 4 -> Best Threshold: 1.0045, Variance Reduction: 0.000013\n",
            "  Feature 5 -> Best Threshold: 0.0921, Variance Reduction: 0.000007\n",
            "  Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.017737\n",
            "  Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.034647\n",
            "Depth 0: Selected Feature 5 at Threshold 0.09205300097808652, Variance Reduction: 0.000007\n",
            "  Feature evaluation at depth 1:\n",
            "    Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.008480\n",
            "    Feature 1 -> Best Threshold: 1.6973, Variance Reduction: 0.000047\n",
            "    Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.005996\n",
            "    Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 0.000096\n",
            "    Feature 4 -> Best Threshold: 1.0045, Variance Reduction: 0.000013\n",
            "    Feature 5 -> Best Threshold: -0.1341, Variance Reduction: 0.001665\n",
            "    Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.018213\n",
            "    Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.035584\n",
            "  Depth 1: Selected Feature 4 at Threshold 1.0044934759693864, Variance Reduction: 0.000013\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.008620\n",
            "      Feature 1 -> Best Threshold: 1.6973, Variance Reduction: 0.000042\n",
            "      Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.006098\n",
            "      Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 0.000096\n",
            "      Feature 4 -> Best Threshold: -1.1475, Variance Reduction: 0.000025\n",
            "      Feature 5 -> Best Threshold: -0.1341, Variance Reduction: 0.000770\n",
            "      Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.018518\n",
            "      Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.036183\n",
            "    Depth 2: Selected Feature 4 at Threshold -1.1475070114279164, Variance Reduction: 0.000025\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.315256\n",
            "        Feature 1 -> Best Threshold: 0.1081, Variance Reduction: 0.114808\n",
            "        Feature 2 -> Best Threshold: 0.0753, Variance Reduction: 0.007140\n",
            "        Feature 3 -> Best Threshold: -0.0417, Variance Reduction: 0.007140\n",
            "        Feature 4 -> Best Threshold: -1.2429, Variance Reduction: 0.315256\n",
            "        Feature 5 -> Best Threshold: -0.0949, Variance Reduction: 0.000990\n",
            "        Feature 6 -> Best Threshold: 1.0151, Variance Reduction: 0.061716\n",
            "        Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 1.576280\n",
            "      Depth 3: Selected Feature 5 at Threshold -0.09489389070884424, Variance Reduction: 0.000990\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -0.6886, Variance Reduction: 1.890625\n",
            "          Feature 1 -> Best Threshold: -2.1167, Variance Reduction: 1.890625\n",
            "          Feature 2 -> Best Threshold: -1.0739, Variance Reduction: 1.890625\n",
            "          Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 1.890625\n",
            "          Feature 4 -> Best Threshold: -1.1758, Variance Reduction: 1.890625\n",
            "          Feature 5 -> Best Threshold: -0.1369, Variance Reduction: 1.890625\n",
            "          Feature 6 -> Best Threshold: 1.0245, Variance Reduction: 1.890625\n",
            "          Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 1.890625\n",
            "        Depth 4: Selected Feature 0 at Threshold -0.6885923462662866, Variance Reduction: 1.890625\n",
            "          Leaf node created at depth 5: Value = 0.6000\n",
            "          Leaf node created at depth 5: Value = 3.3500\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.506969\n",
            "          Feature 1 -> Best Threshold: -1.4811, Variance Reduction: 0.094785\n",
            "          Feature 2 -> Best Threshold: 0.0753, Variance Reduction: 0.447181\n",
            "          Feature 3 -> Best Threshold: -0.0417, Variance Reduction: 0.447181\n",
            "          Feature 4 -> Best Threshold: -1.2429, Variance Reduction: 0.506969\n",
            "          Feature 5 -> Best Threshold: 0.0129, Variance Reduction: 0.447181\n",
            "          Feature 6 -> Best Threshold: 1.0151, Variance Reduction: 0.094785\n",
            "          Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 1.429818\n",
            "        Depth 4: Selected Feature 1 at Threshold -1.481057777728126, Variance Reduction: 0.094785\n",
            "          Leaf node created at depth 5: Value = 1.3750\n",
            "          Leaf node created at depth 5: Value = 2.0860\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -1.6374, Variance Reduction: 0.004886\n",
            "        Feature 1 -> Best Threshold: -0.6070, Variance Reduction: 0.000006\n",
            "        Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.000311\n",
            "        Feature 3 -> Best Threshold: -0.3569, Variance Reduction: 0.000075\n",
            "        Feature 4 -> Best Threshold: 0.1780, Variance Reduction: 0.000452\n",
            "        Feature 5 -> Best Threshold: -0.1573, Variance Reduction: 0.001888\n",
            "        Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.019156\n",
            "        Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.037440\n",
            "      Depth 3: Selected Feature 1 at Threshold -0.6070189133741593, Variance Reduction: 0.000006\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.6374, Variance Reduction: 0.011650\n",
            "          Feature 1 -> Best Threshold: -0.7659, Variance Reduction: 0.000048\n",
            "          Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.005868\n",
            "          Feature 3 -> Best Threshold: -0.0889, Variance Reduction: 0.013689\n",
            "          Feature 4 -> Best Threshold: -0.6221, Variance Reduction: 0.000484\n",
            "          Feature 5 -> Best Threshold: -0.0857, Variance Reduction: 0.000005\n",
            "          Feature 6 -> Best Threshold: 1.0151, Variance Reduction: 0.078764\n",
            "          Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 0.036583\n",
            "        Depth 4: Selected Feature 5 at Threshold -0.08571359033943203, Variance Reduction: 0.000005\n",
            "          Leaf node created at depth 5: Value = 1.9507\n",
            "          Leaf node created at depth 5: Value = 1.9460\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.6539, Variance Reduction: 0.005801\n",
            "          Feature 1 -> Best Threshold: -0.2097, Variance Reduction: 0.000009\n",
            "          Feature 2 -> Best Threshold: -1.2583, Variance Reduction: 0.003402\n",
            "          Feature 3 -> Best Threshold: -0.3569, Variance Reduction: 0.000078\n",
            "          Feature 4 -> Best Threshold: 0.1780, Variance Reduction: 0.000046\n",
            "          Feature 5 -> Best Threshold: -0.1573, Variance Reduction: 0.001992\n",
            "          Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.020285\n",
            "          Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.039669\n",
            "        Depth 4: Selected Feature 1 at Threshold -0.20972852048599255, Variance Reduction: 0.000009\n",
            "          Leaf node created at depth 5: Value = 1.9424\n",
            "          Leaf node created at depth 5: Value = 1.9601\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: -0.9796, Variance Reduction: 0.197401\n",
            "      Feature 1 -> Best Threshold: 0.5849, Variance Reduction: 0.320267\n",
            "      Feature 2 -> Best Threshold: -1.2671, Variance Reduction: 0.197401\n",
            "      Feature 3 -> Best Threshold: -0.0641, Variance Reduction: 0.197401\n",
            "      Feature 4 -> Best Threshold: 1.8045, Variance Reduction: 0.320267\n",
            "      Feature 5 -> Best Threshold: -0.1519, Variance Reduction: 0.197401\n",
            "      Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 0.320267\n",
            "      Feature 7 -> Best Threshold: -1.3428, Variance Reduction: 0.320267\n",
            "    Depth 2: Selected Feature 0 at Threshold -0.9796267044691515, Variance Reduction: 0.197401\n",
            "      Leaf node created at depth 3: Value = 1.3000\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -0.8838, Variance Reduction: 1.242110\n",
            "        Feature 1 -> Best Threshold: 0.4259, Variance Reduction: 1.242110\n",
            "        Feature 2 -> Best Threshold: -0.8679, Variance Reduction: 1.242110\n",
            "        Feature 3 -> Best Threshold: -0.0566, Variance Reduction: 1.242110\n",
            "        Feature 4 -> Best Threshold: 1.1158, Variance Reduction: 1.242110\n",
            "        Feature 5 -> Best Threshold: -0.1509, Variance Reduction: 1.242110\n",
            "        Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 1.242110\n",
            "        Feature 7 -> Best Threshold: -1.3428, Variance Reduction: 1.242110\n",
            "      Depth 3: Selected Feature 0 at Threshold -0.8837732831836718, Variance Reduction: 1.242110\n",
            "        Leaf node created at depth 4: Value = 1.1280\n",
            "        Leaf node created at depth 4: Value = 3.3570\n",
            "  Feature evaluation at depth 1:\n",
            "    Feature 0 -> Best Threshold: -1.6427, Variance Reduction: 0.024964\n",
            "    Feature 1 -> Best Threshold: 1.4589, Variance Reduction: 0.243050\n",
            "    Feature 2 -> Best Threshold: -1.2068, Variance Reduction: 0.080090\n",
            "    Feature 3 -> Best Threshold: -0.2280, Variance Reduction: 0.243050\n",
            "    Feature 4 -> Best Threshold: -1.0901, Variance Reduction: 0.024964\n",
            "    Feature 5 -> Best Threshold: 0.2452, Variance Reduction: 0.080090\n",
            "    Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 0.789891\n",
            "    Feature 7 -> Best Threshold: -1.3528, Variance Reduction: 0.129655\n",
            "  Depth 1: Selected Feature 0 at Threshold -1.642652346705231, Variance Reduction: 0.024964\n",
            "    Leaf node created at depth 2: Value = 1.6250\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: -1.5812, Variance Reduction: 0.138676\n",
            "      Feature 1 -> Best Threshold: 1.4589, Variance Reduction: 0.378077\n",
            "      Feature 2 -> Best Threshold: -1.2068, Variance Reduction: 0.138676\n",
            "      Feature 3 -> Best Threshold: -0.2280, Variance Reduction: 0.378077\n",
            "      Feature 4 -> Best Threshold: -0.4605, Variance Reduction: 0.378077\n",
            "      Feature 5 -> Best Threshold: 0.1529, Variance Reduction: 0.138676\n",
            "      Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 1.363062\n",
            "      Feature 7 -> Best Threshold: -1.3528, Variance Reduction: 0.138676\n",
            "    Depth 2: Selected Feature 0 at Threshold -1.5811714043321172, Variance Reduction: 0.138676\n",
            "      Leaf node created at depth 3: Value = 1.3750\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -0.9320, Variance Reduction: 0.819204\n",
            "        Feature 1 -> Best Threshold: 1.4589, Variance Reduction: 0.819204\n",
            "        Feature 2 -> Best Threshold: -0.9083, Variance Reduction: 0.819204\n",
            "        Feature 3 -> Best Threshold: -0.3778, Variance Reduction: 0.819204\n",
            "        Feature 4 -> Best Threshold: -0.9091, Variance Reduction: 0.819204\n",
            "        Feature 5 -> Best Threshold: 0.0974, Variance Reduction: 0.819204\n",
            "        Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 3.822631\n",
            "        Feature 7 -> Best Threshold: -1.3378, Variance Reduction: 0.819204\n",
            "      Depth 3: Selected Feature 0 at Threshold -0.9320421394817413, Variance Reduction: 0.819204\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.3829, Variance Reduction: 4.515646\n",
            "          Feature 1 -> Best Threshold: 1.7767, Variance Reduction: 4.515646\n",
            "          Feature 2 -> Best Threshold: -1.0103, Variance Reduction: 4.515646\n",
            "          Feature 3 -> Best Threshold: -0.6644, Variance Reduction: 4.515646\n",
            "          Feature 4 -> Best Threshold: -1.0574, Variance Reduction: 4.515646\n",
            "          Feature 5 -> Best Threshold: 0.1035, Variance Reduction: 4.515646\n",
            "          Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 4.515646\n",
            "          Feature 7 -> Best Threshold: -1.3478, Variance Reduction: 4.515646\n",
            "        Depth 4: Selected Feature 0 at Threshold -1.3829374754133272, Variance Reduction: 4.515646\n",
            "          Leaf node created at depth 5: Value = 5.0000\n",
            "          Leaf node created at depth 5: Value = 0.7500\n",
            "        Leaf node created at depth 4: Value = 0.9550\n",
            "\n",
            "Finished building the Regression Tree!\n",
            "\n",
            "\n",
            "Mean Squared Error on Test Set: 1.2838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check against the Scikit-learn evaluation."
      ],
      "metadata": {
        "id": "k-oxwmlkUqXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree = DecisionTreeRegressor(max_depth=5, ccp_alpha=0.01, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "predictions = tree.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f\"\\nMean Squared Error on Test Set: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "c7CQmrmhUk2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b00a634-0150-4170-cf3f-c9ea239eb764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Squared Error on Test Set: 0.6846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Decision trees are one of the most intuitive and powerful machine learning models, providing clear decision-making structures for both **classification** and **regression** problems. By recursively splitting data into subsets, decision trees create a hierarchical model that captures complex relationships in the data.\n",
        "\n",
        "What we have learned:\n",
        "\n",
        "- **Classification Trees** use impurity measures such as **Gini Impurity** and **Entropy** to find the best splits, ensuring that classes are well separated.\n",
        "- **Regression Trees** use **variance reduction** to minimize prediction error, selecting splits that reduce the spread of target values within each node.\n",
        "- **Information Gain** plays a crucial role in selecting the best feature for splitting, ensuring that each decision node maximally improves homogeneity.\n",
        "- **Pruning Techniques** help prevent overfitting by removing unnecessary branches and ensuring the model generalizes well to new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "-SzcFKjfRcE6"
      }
    }
  ]
}
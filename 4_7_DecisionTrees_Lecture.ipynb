{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamd1985/Lectures_On_MLAI/blob/main/4_7_DecisionTrees_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb1hgpq992UR"
      },
      "source": [
        "# Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9czgntjHFFTg"
      },
      "source": [
        "##Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvP8QXcn92US"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEwmX3Xi92UY"
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, prediction=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.prediction = prediction  # This is only used if the node is a leaf.\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.prediction is not None\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, cost_function=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.cost_function = cost_function\n",
        "        self.root = None  # Root node of the tree.\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Starts building the decision tree.\"\"\"\n",
        "        print(\"\\nStarting to build the Decision Tree...\\n\")\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        print(\"\\nFinished building the Decision Tree!\\n\")\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"Recursively builds the decision tree.\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # Check stopping conditions\n",
        "        if len(unique_classes) == 1 or (self.max_depth and depth >= self.max_depth):\n",
        "            leaf_class = Counter(y).most_common(1)[0][0]\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Class = {leaf_class}\")\n",
        "            return Node(prediction=leaf_class)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold, feature_scores = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            leaf_class = Counter(y).most_common(1)[0][0]\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Class = {leaf_class}\")\n",
        "            return Node(prediction=leaf_class)\n",
        "\n",
        "        # Print feature scores before selecting the best one\n",
        "        print(f\"{'  ' * depth}Feature evaluation at depth {depth}:\")\n",
        "        for feat, (thresh, score) in feature_scores.items():\n",
        "            print(f\"{'  ' * depth}  Feature {feat} -> Best Threshold: {thresh:.4f}, Score: {score:.6f}\")\n",
        "\n",
        "        print(f\"{'  ' * depth}Depth {depth}: Selected Feature {best_feature} at Threshold {best_threshold}\")\n",
        "\n",
        "        # Split dataset\n",
        "        left_indices = X[:, best_feature] <= best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"Finds the best feature and threshold to split the dataset, while logging scores.\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        best_gain = -1\n",
        "        best_feature, best_threshold = None, None\n",
        "        feature_scores = {}\n",
        "\n",
        "        for feature in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            best_feature_score = -1\n",
        "            best_feature_threshold = None\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_y = y[X[:, feature] <= threshold]\n",
        "                right_y = y[X[:, feature] > threshold]\n",
        "\n",
        "                if len(left_y) == 0 or len(right_y) == 0:\n",
        "                    continue\n",
        "\n",
        "                gain = self._information_gain(y, left_y, right_y)\n",
        "                if gain > best_feature_score:\n",
        "                    best_feature_score = gain\n",
        "                    best_feature_threshold = threshold\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain, best_feature, best_threshold = gain, feature, threshold\n",
        "\n",
        "            # Store the best threshold and gain score for this feature\n",
        "            feature_scores[feature] = (best_feature_threshold, best_feature_score)\n",
        "\n",
        "        return best_feature, best_threshold, feature_scores\n",
        "\n",
        "    def _information_gain(self, y, left_y, right_y):\n",
        "        \"\"\"Computes information gain based on the chosen cost function.\"\"\"\n",
        "        weight_left = len(left_y) / len(y)\n",
        "        weight_right = len(right_y) / len(y)\n",
        "\n",
        "        return self.cost_function(y) - (weight_left * self.cost_function(left_y) + weight_right * self.cost_function(right_y))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the class labels for a given dataset.\"\"\"\n",
        "        return np.array([self._traverse_tree(sample, self.root) for sample in X])\n",
        "\n",
        "    def _traverse_tree(self, sample, node):\n",
        "        \"\"\"Recursively traverses the tree to make predictions.\"\"\"\n",
        "        if node.is_leaf():\n",
        "            return node.prediction\n",
        "        if sample[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(sample, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(sample, node.right)\n"
      ],
      "metadata": {
        "id": "EpNuGopd3Kmn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    return 1 - np.sum(probabilities ** 2)\n",
        "\n",
        "def entropy(y):\n",
        "    _, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    return -np.sum(probabilities * np.log2(probabilities + 1e-10))"
      ],
      "metadata": {
        "id": "cXfsmKTk4Pxe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "tree = DecisionTree(max_depth=3, cost_function=gini_impurity)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "predictions = tree.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TnVJeFQ4Rvv",
        "outputId": "9abcd04c-dc61-4bda-c41a-41f104abccb3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting to build the Decision Tree...\n",
            "\n",
            "Feature evaluation at depth 0:\n",
            "  Feature 0 -> Best Threshold: 5.4000, Score: 0.226456\n",
            "  Feature 1 -> Best Threshold: 3.3000, Score: 0.135683\n",
            "  Feature 2 -> Best Threshold: 1.9000, Score: 0.333403\n",
            "  Feature 3 -> Best Threshold: 0.6000, Score: 0.333403\n",
            "Depth 0: Selected Feature 2 at Threshold 1.9\n",
            "  Leaf node created at depth 1: Class = 0\n",
            "  Feature evaluation at depth 1:\n",
            "    Feature 0 -> Best Threshold: 6.1000, Score: 0.091072\n",
            "    Feature 1 -> Best Threshold: 2.4000, Score: 0.035916\n",
            "    Feature 2 -> Best Threshold: 4.7000, Score: 0.364898\n",
            "    Feature 3 -> Best Threshold: 1.7000, Score: 0.364291\n",
            "  Depth 1: Selected Feature 2 at Threshold 4.7\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: 4.9000, Score: 0.025566\n",
            "      Feature 1 -> Best Threshold: 2.5000, Score: 0.003453\n",
            "      Feature 2 -> Best Threshold: 4.4000, Score: 0.003453\n",
            "      Feature 3 -> Best Threshold: 1.6000, Score: 0.052593\n",
            "    Depth 2: Selected Feature 3 at Threshold 1.6\n",
            "      Leaf node created at depth 3: Class = 1\n",
            "      Leaf node created at depth 3: Class = 2\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: 6.9000, Score: 0.008194\n",
            "      Feature 1 -> Best Threshold: 3.2000, Score: 0.005258\n",
            "      Feature 2 -> Best Threshold: 5.0000, Score: 0.042065\n",
            "      Feature 3 -> Best Threshold: 1.7000, Score: 0.067311\n",
            "    Depth 2: Selected Feature 3 at Threshold 1.7\n",
            "      Leaf node created at depth 3: Class = 2\n",
            "      Leaf node created at depth 3: Class = 2\n",
            "\n",
            "Finished building the Decision Tree!\n",
            "\n",
            "Decision Tree Accuracy: 0.9666666666666667\n",
            "Decision Tree Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      0.89      0.94         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PrunedDecisionTree(DecisionTree):\n",
        "    def __init__(self, max_depth=None, cost_function=None, alpha=0.01):\n",
        "        \"\"\"\n",
        "        Pruned Decision Tree constructor.\n",
        "\n",
        "        :param max_depth: Maximum depth of the tree.\n",
        "        :param cost_function: A function to compute impurity (e.g., Gini or Entropy).\n",
        "        :param alpha: Regularization parameter for pruning.\n",
        "        \"\"\"\n",
        "        super().__init__(max_depth, cost_function)\n",
        "        self.alpha = alpha  # Regularization parameter for pruning\n",
        "\n",
        "    def _evaluate_accuracy(self, X_val, y_val):\n",
        "        \"\"\"Computes accuracy of the tree on validation data.\"\"\"\n",
        "        predictions = self.predict(X_val)\n",
        "        return np.mean(predictions == y_val)\n",
        "\n",
        "\n",
        "    def prune(self, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Prunes the tree using cost-complexity pruning.\n",
        "\n",
        "        :param X_val: Validation feature set for pruning evaluation.\n",
        "        :param y_val: Validation labels.\n",
        "        \"\"\"\n",
        "        print(\"\\nStarting pruning process...\\n\")\n",
        "        self._prune_tree(self.root, X_val, y_val, depth=0)\n",
        "        print(\"\\nPruning completed!\\n\")\n",
        "\n",
        "    def _prune_tree(self, node, X_val, y_val, depth):\n",
        "      \"\"\"\n",
        "      Recursively prunes the tree using validation data.\n",
        "\n",
        "      :param node: The current node in the tree.\n",
        "      :param X_val: Validation feature set.\n",
        "      :param y_val: Validation labels.\n",
        "      :param depth: Depth of the current node in the tree.\n",
        "      \"\"\"\n",
        "      if node.is_leaf():\n",
        "          return\n",
        "\n",
        "      # Prune left and right children recursively\n",
        "      self._prune_tree(node.left, X_val, y_val, depth + 1)\n",
        "      self._prune_tree(node.right, X_val, y_val, depth + 1)\n",
        "\n",
        "      # Check if the node can be pruned\n",
        "      if node.left.is_leaf() and node.right.is_leaf():\n",
        "          # Compute validation accuracy before pruning\n",
        "          pre_prune_accuracy = self._evaluate_accuracy(X_val, y_val)\n",
        "\n",
        "          # Save the original node structure\n",
        "          left_node, right_node = node.left, node.right\n",
        "\n",
        "          # Convert node into a leaf (majority class)\n",
        "          node.prediction = Counter(y_val).most_common(1)[0][0]\n",
        "          node.left = None\n",
        "          node.right = None\n",
        "\n",
        "          # Compute validation accuracy after pruning\n",
        "          post_prune_accuracy = self._evaluate_accuracy(X_val, y_val)\n",
        "\n",
        "          # Apply pruning only if accuracy does NOT drop significantly\n",
        "          if post_prune_accuracy < pre_prune_accuracy - self.alpha:\n",
        "              # Revert pruning\n",
        "              node.prediction = None\n",
        "              node.left = left_node\n",
        "              node.right = right_node\n",
        "              print(f\"{'  ' * depth}Retained split at depth {depth}, feature {node.feature}, threshold {node.threshold}\")\n",
        "          else:\n",
        "              print(f\"{'  ' * depth}Pruned subtree at depth {depth}, feature {node.feature}, threshold {node.threshold}\")\n"
      ],
      "metadata": {
        "id": "nSwqHDMg6rH1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train tree with Gini impurity\n",
        "tree = PrunedDecisionTree(max_depth=3, cost_function=gini_impurity, alpha=0.01)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Prune the tree\n",
        "tree.prune(X_val, y_val)\n",
        "\n",
        "# Predict and evaluate\n",
        "predictions = tree.predict(X_val)\n",
        "accuracy = np.mean(predictions == y_val)\n",
        "print(f\"Final Accuracy after pruning: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFRm9tJU6tWu",
        "outputId": "3bd788ba-9f36-494a-9a8e-f48fb7b9fb68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting to build the Decision Tree...\n",
            "\n",
            "Feature evaluation at depth 0:\n",
            "  Feature 0 -> Best Threshold: 5.4000, Score: 0.226456\n",
            "  Feature 1 -> Best Threshold: 3.3000, Score: 0.135683\n",
            "  Feature 2 -> Best Threshold: 1.9000, Score: 0.333403\n",
            "  Feature 3 -> Best Threshold: 0.6000, Score: 0.333403\n",
            "Depth 0: Selected Feature 2 at Threshold 1.9\n",
            "  Leaf node created at depth 1: Class = 0\n",
            "  Feature evaluation at depth 1:\n",
            "    Feature 0 -> Best Threshold: 6.1000, Score: 0.091072\n",
            "    Feature 1 -> Best Threshold: 2.4000, Score: 0.035916\n",
            "    Feature 2 -> Best Threshold: 4.7000, Score: 0.364898\n",
            "    Feature 3 -> Best Threshold: 1.7000, Score: 0.364291\n",
            "  Depth 1: Selected Feature 2 at Threshold 4.7\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: 4.9000, Score: 0.025566\n",
            "      Feature 1 -> Best Threshold: 2.5000, Score: 0.003453\n",
            "      Feature 2 -> Best Threshold: 4.4000, Score: 0.003453\n",
            "      Feature 3 -> Best Threshold: 1.6000, Score: 0.052593\n",
            "    Depth 2: Selected Feature 3 at Threshold 1.6\n",
            "      Leaf node created at depth 3: Class = 1\n",
            "      Leaf node created at depth 3: Class = 2\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: 6.9000, Score: 0.008194\n",
            "      Feature 1 -> Best Threshold: 3.2000, Score: 0.005258\n",
            "      Feature 2 -> Best Threshold: 5.0000, Score: 0.042065\n",
            "      Feature 3 -> Best Threshold: 1.7000, Score: 0.067311\n",
            "    Depth 2: Selected Feature 3 at Threshold 1.7\n",
            "      Leaf node created at depth 3: Class = 2\n",
            "      Leaf node created at depth 3: Class = 2\n",
            "\n",
            "Finished building the Decision Tree!\n",
            "\n",
            "\n",
            "Starting pruning process...\n",
            "\n",
            "    Retained split at depth 2, feature 3, threshold 1.6\n",
            "    Pruned subtree at depth 2, feature 3, threshold 1.7\n",
            "\n",
            "Pruning completed!\n",
            "\n",
            "Final Accuracy after pruning: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class RegressionTree:\n",
        "    def __init__(self, max_depth=5):\n",
        "        \"\"\"\n",
        "        Regression Tree constructor.\n",
        "\n",
        "        :param max_depth: Maximum depth of the tree.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Starts building the regression tree.\"\"\"\n",
        "        print(\"\\nStarting to build the Regression Tree...\\n\")\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        print(\"\\nFinished building the Regression Tree!\\n\")\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"Recursively builds the regression tree.\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        # Stopping condition: If only one unique value left or max depth reached, create a leaf\n",
        "        if len(np.unique(y)) == 1 or (self.max_depth and depth >= self.max_depth):\n",
        "            leaf_value = np.mean(y)  # Leaf stores mean value for regression\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Value = {leaf_value:.4f}\")\n",
        "            return Node(prediction=leaf_value)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold, best_score, feature_scores = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            leaf_value = np.mean(y)\n",
        "            print(f\"{'  ' * depth}Leaf node created at depth {depth}: Value = {leaf_value:.4f}\")\n",
        "            return Node(prediction=leaf_value)\n",
        "\n",
        "        # Print feature scores before selecting the best one\n",
        "        print(f\"{'  ' * depth}Feature evaluation at depth {depth}:\")\n",
        "        for feat, (thresh, score) in feature_scores.items():\n",
        "            print(f\"{'  ' * depth}  Feature {feat} -> Best Threshold: {thresh:.4f}, Variance Reduction: {score:.6f}\")\n",
        "\n",
        "        print(f\"{'  ' * depth}Depth {depth}: Selected Feature {best_feature} at Threshold {best_threshold}, Variance Reduction: {best_score:.6f}\")\n",
        "\n",
        "        # Split dataset\n",
        "        left_indices = X[:, best_feature] <= best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"Finds the best feature and threshold to split the dataset, while logging scores.\"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        best_score = float(\"inf\")  # Lower variance is better\n",
        "        best_feature, best_threshold = None, None\n",
        "        feature_scores = {}\n",
        "\n",
        "        for feature in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            best_feature_score = float(\"inf\")\n",
        "            best_feature_threshold = None\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_y = y[X[:, feature] <= threshold]\n",
        "                right_y = y[X[:, feature] > threshold]\n",
        "\n",
        "                if len(left_y) == 0 or len(right_y) == 0:\n",
        "                    continue\n",
        "\n",
        "                score = self._variance_reduction(y, left_y, right_y)\n",
        "                if score < best_feature_score:  # Lower variance is better\n",
        "                    best_feature_score = score\n",
        "                    best_feature_threshold = threshold\n",
        "\n",
        "                if score < best_score:\n",
        "                    best_score, best_feature, best_threshold = score, feature, threshold\n",
        "\n",
        "            # Store the best threshold and variance reduction score for this feature\n",
        "            feature_scores[feature] = (best_feature_threshold, best_feature_score)\n",
        "\n",
        "        return best_feature, best_threshold, best_score, feature_scores\n",
        "\n",
        "    def _variance_reduction(self, y, left_y, right_y):\n",
        "        \"\"\"Computes variance reduction (MSE reduction).\"\"\"\n",
        "        weight_left = len(left_y) / len(y)\n",
        "        weight_right = len(right_y) / len(y)\n",
        "\n",
        "        return np.var(y) - (weight_left * np.var(left_y) + weight_right * np.var(right_y))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the target values for a given dataset.\"\"\"\n",
        "        return np.array([self._traverse_tree(sample, self.root) for sample in X])\n",
        "\n",
        "    def _traverse_tree(self, sample, node):\n",
        "        \"\"\"Recursively traverses the tree to make predictions.\"\"\"\n",
        "        if node.is_leaf():\n",
        "            return node.prediction\n",
        "        if sample[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(sample, node.left)\n",
        "        else:\n",
        "            return self._traverse_tree(sample, node.right)\n"
      ],
      "metadata": {
        "id": "eWKGVUcK9JHD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Select first 100 for training, last 20 for testing\n",
        "X_train, y_train = X[:200], y[:200]\n",
        "X_test, y_test = X[-20:], y[-20:]\n",
        "\n",
        "# Train regression tree\n",
        "tree = RegressionTree(max_depth=5)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = tree.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f\"\\nMean Squared Error on Test Set: {mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bedOeom59o8A",
        "outputId": "7fa58e8d-b683-4a72-e17c-f98bc74742d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting to build the Regression Tree...\n",
            "\n",
            "Feature evaluation at depth 0:\n",
            "  Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.008262\n",
            "  Feature 1 -> Best Threshold: 1.6178, Variance Reduction: 0.000050\n",
            "  Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.005838\n",
            "  Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 0.000095\n",
            "  Feature 4 -> Best Threshold: 1.0045, Variance Reduction: 0.000013\n",
            "  Feature 5 -> Best Threshold: 0.0921, Variance Reduction: 0.000007\n",
            "  Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.017737\n",
            "  Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.034647\n",
            "Depth 0: Selected Feature 5 at Threshold 0.09205300097808652, Variance Reduction: 0.000007\n",
            "  Feature evaluation at depth 1:\n",
            "    Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.008480\n",
            "    Feature 1 -> Best Threshold: 1.6973, Variance Reduction: 0.000047\n",
            "    Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.005996\n",
            "    Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 0.000096\n",
            "    Feature 4 -> Best Threshold: 1.0045, Variance Reduction: 0.000013\n",
            "    Feature 5 -> Best Threshold: -0.1341, Variance Reduction: 0.001665\n",
            "    Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.018213\n",
            "    Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.035584\n",
            "  Depth 1: Selected Feature 4 at Threshold 1.0044934759693864, Variance Reduction: 0.000013\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.008620\n",
            "      Feature 1 -> Best Threshold: 1.6973, Variance Reduction: 0.000042\n",
            "      Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.006098\n",
            "      Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 0.000096\n",
            "      Feature 4 -> Best Threshold: -1.1475, Variance Reduction: 0.000025\n",
            "      Feature 5 -> Best Threshold: -0.1341, Variance Reduction: 0.000770\n",
            "      Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.018518\n",
            "      Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.036183\n",
            "    Depth 2: Selected Feature 4 at Threshold -1.1475070114279164, Variance Reduction: 0.000025\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.315256\n",
            "        Feature 1 -> Best Threshold: 0.1081, Variance Reduction: 0.114808\n",
            "        Feature 2 -> Best Threshold: 0.0753, Variance Reduction: 0.007140\n",
            "        Feature 3 -> Best Threshold: -0.0417, Variance Reduction: 0.007140\n",
            "        Feature 4 -> Best Threshold: -1.2429, Variance Reduction: 0.315256\n",
            "        Feature 5 -> Best Threshold: -0.0949, Variance Reduction: 0.000990\n",
            "        Feature 6 -> Best Threshold: 1.0151, Variance Reduction: 0.061716\n",
            "        Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 1.576280\n",
            "      Depth 3: Selected Feature 5 at Threshold -0.09489389070884424, Variance Reduction: 0.000990\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -0.6886, Variance Reduction: 1.890625\n",
            "          Feature 1 -> Best Threshold: -2.1167, Variance Reduction: 1.890625\n",
            "          Feature 2 -> Best Threshold: -1.0739, Variance Reduction: 1.890625\n",
            "          Feature 3 -> Best Threshold: -0.7223, Variance Reduction: 1.890625\n",
            "          Feature 4 -> Best Threshold: -1.1758, Variance Reduction: 1.890625\n",
            "          Feature 5 -> Best Threshold: -0.1369, Variance Reduction: 1.890625\n",
            "          Feature 6 -> Best Threshold: 1.0245, Variance Reduction: 1.890625\n",
            "          Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 1.890625\n",
            "        Depth 4: Selected Feature 0 at Threshold -0.6885923462662866, Variance Reduction: 1.890625\n",
            "          Leaf node created at depth 5: Value = 0.6000\n",
            "          Leaf node created at depth 5: Value = 3.3500\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.7743, Variance Reduction: 0.506969\n",
            "          Feature 1 -> Best Threshold: -1.4811, Variance Reduction: 0.094785\n",
            "          Feature 2 -> Best Threshold: 0.0753, Variance Reduction: 0.447181\n",
            "          Feature 3 -> Best Threshold: -0.0417, Variance Reduction: 0.447181\n",
            "          Feature 4 -> Best Threshold: -1.2429, Variance Reduction: 0.506969\n",
            "          Feature 5 -> Best Threshold: 0.0129, Variance Reduction: 0.447181\n",
            "          Feature 6 -> Best Threshold: 1.0151, Variance Reduction: 0.094785\n",
            "          Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 1.429818\n",
            "        Depth 4: Selected Feature 1 at Threshold -1.481057777728126, Variance Reduction: 0.094785\n",
            "          Leaf node created at depth 5: Value = 1.3750\n",
            "          Leaf node created at depth 5: Value = 2.0860\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -1.6374, Variance Reduction: 0.004886\n",
            "        Feature 1 -> Best Threshold: -0.6070, Variance Reduction: 0.000006\n",
            "        Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.000311\n",
            "        Feature 3 -> Best Threshold: -0.3569, Variance Reduction: 0.000075\n",
            "        Feature 4 -> Best Threshold: 0.1780, Variance Reduction: 0.000452\n",
            "        Feature 5 -> Best Threshold: -0.1573, Variance Reduction: 0.001888\n",
            "        Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.019156\n",
            "        Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.037440\n",
            "      Depth 3: Selected Feature 1 at Threshold -0.6070189133741593, Variance Reduction: 0.000006\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.6374, Variance Reduction: 0.011650\n",
            "          Feature 1 -> Best Threshold: -0.7659, Variance Reduction: 0.000048\n",
            "          Feature 2 -> Best Threshold: -1.3675, Variance Reduction: 0.005868\n",
            "          Feature 3 -> Best Threshold: -0.0889, Variance Reduction: 0.013689\n",
            "          Feature 4 -> Best Threshold: -0.6221, Variance Reduction: 0.000484\n",
            "          Feature 5 -> Best Threshold: -0.0857, Variance Reduction: 0.000005\n",
            "          Feature 6 -> Best Threshold: 1.0151, Variance Reduction: 0.078764\n",
            "          Feature 7 -> Best Threshold: -1.3578, Variance Reduction: 0.036583\n",
            "        Depth 4: Selected Feature 5 at Threshold -0.08571359033943203, Variance Reduction: 0.000005\n",
            "          Leaf node created at depth 5: Value = 1.9507\n",
            "          Leaf node created at depth 5: Value = 1.9460\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.6539, Variance Reduction: 0.005801\n",
            "          Feature 1 -> Best Threshold: -0.2097, Variance Reduction: 0.000009\n",
            "          Feature 2 -> Best Threshold: -1.2583, Variance Reduction: 0.003402\n",
            "          Feature 3 -> Best Threshold: -0.3569, Variance Reduction: 0.000078\n",
            "          Feature 4 -> Best Threshold: 0.1780, Variance Reduction: 0.000046\n",
            "          Feature 5 -> Best Threshold: -0.1573, Variance Reduction: 0.001992\n",
            "          Feature 6 -> Best Threshold: 1.0010, Variance Reduction: 0.020285\n",
            "          Feature 7 -> Best Threshold: -1.3628, Variance Reduction: 0.039669\n",
            "        Depth 4: Selected Feature 1 at Threshold -0.20972852048599255, Variance Reduction: 0.000009\n",
            "          Leaf node created at depth 5: Value = 1.9424\n",
            "          Leaf node created at depth 5: Value = 1.9601\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: -0.9796, Variance Reduction: 0.197401\n",
            "      Feature 1 -> Best Threshold: 0.5849, Variance Reduction: 0.320267\n",
            "      Feature 2 -> Best Threshold: -1.2671, Variance Reduction: 0.197401\n",
            "      Feature 3 -> Best Threshold: -0.0641, Variance Reduction: 0.197401\n",
            "      Feature 4 -> Best Threshold: 1.8045, Variance Reduction: 0.320267\n",
            "      Feature 5 -> Best Threshold: -0.1519, Variance Reduction: 0.197401\n",
            "      Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 0.320267\n",
            "      Feature 7 -> Best Threshold: -1.3428, Variance Reduction: 0.320267\n",
            "    Depth 2: Selected Feature 0 at Threshold -0.9796267044691515, Variance Reduction: 0.197401\n",
            "      Leaf node created at depth 3: Value = 1.3000\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -0.8838, Variance Reduction: 1.242110\n",
            "        Feature 1 -> Best Threshold: 0.4259, Variance Reduction: 1.242110\n",
            "        Feature 2 -> Best Threshold: -0.8679, Variance Reduction: 1.242110\n",
            "        Feature 3 -> Best Threshold: -0.0566, Variance Reduction: 1.242110\n",
            "        Feature 4 -> Best Threshold: 1.1158, Variance Reduction: 1.242110\n",
            "        Feature 5 -> Best Threshold: -0.1509, Variance Reduction: 1.242110\n",
            "        Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 1.242110\n",
            "        Feature 7 -> Best Threshold: -1.3428, Variance Reduction: 1.242110\n",
            "      Depth 3: Selected Feature 0 at Threshold -0.8837732831836718, Variance Reduction: 1.242110\n",
            "        Leaf node created at depth 4: Value = 1.1280\n",
            "        Leaf node created at depth 4: Value = 3.3570\n",
            "  Feature evaluation at depth 1:\n",
            "    Feature 0 -> Best Threshold: -1.6427, Variance Reduction: 0.024964\n",
            "    Feature 1 -> Best Threshold: 1.4589, Variance Reduction: 0.243050\n",
            "    Feature 2 -> Best Threshold: -1.2068, Variance Reduction: 0.080090\n",
            "    Feature 3 -> Best Threshold: -0.2280, Variance Reduction: 0.243050\n",
            "    Feature 4 -> Best Threshold: -1.0901, Variance Reduction: 0.024964\n",
            "    Feature 5 -> Best Threshold: 0.2452, Variance Reduction: 0.080090\n",
            "    Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 0.789891\n",
            "    Feature 7 -> Best Threshold: -1.3528, Variance Reduction: 0.129655\n",
            "  Depth 1: Selected Feature 0 at Threshold -1.642652346705231, Variance Reduction: 0.024964\n",
            "    Leaf node created at depth 2: Value = 1.6250\n",
            "    Feature evaluation at depth 2:\n",
            "      Feature 0 -> Best Threshold: -1.5812, Variance Reduction: 0.138676\n",
            "      Feature 1 -> Best Threshold: 1.4589, Variance Reduction: 0.378077\n",
            "      Feature 2 -> Best Threshold: -1.2068, Variance Reduction: 0.138676\n",
            "      Feature 3 -> Best Threshold: -0.2280, Variance Reduction: 0.378077\n",
            "      Feature 4 -> Best Threshold: -0.4605, Variance Reduction: 0.378077\n",
            "      Feature 5 -> Best Threshold: 0.1529, Variance Reduction: 0.138676\n",
            "      Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 1.363062\n",
            "      Feature 7 -> Best Threshold: -1.3528, Variance Reduction: 0.138676\n",
            "    Depth 2: Selected Feature 0 at Threshold -1.5811714043321172, Variance Reduction: 0.138676\n",
            "      Leaf node created at depth 3: Value = 1.3750\n",
            "      Feature evaluation at depth 3:\n",
            "        Feature 0 -> Best Threshold: -0.9320, Variance Reduction: 0.819204\n",
            "        Feature 1 -> Best Threshold: 1.4589, Variance Reduction: 0.819204\n",
            "        Feature 2 -> Best Threshold: -0.9083, Variance Reduction: 0.819204\n",
            "        Feature 3 -> Best Threshold: -0.3778, Variance Reduction: 0.819204\n",
            "        Feature 4 -> Best Threshold: -0.9091, Variance Reduction: 0.819204\n",
            "        Feature 5 -> Best Threshold: 0.0974, Variance Reduction: 0.819204\n",
            "        Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 3.822631\n",
            "        Feature 7 -> Best Threshold: -1.3378, Variance Reduction: 0.819204\n",
            "      Depth 3: Selected Feature 0 at Threshold -0.9320421394817413, Variance Reduction: 0.819204\n",
            "        Feature evaluation at depth 4:\n",
            "          Feature 0 -> Best Threshold: -1.3829, Variance Reduction: 4.515646\n",
            "          Feature 1 -> Best Threshold: 1.7767, Variance Reduction: 4.515646\n",
            "          Feature 2 -> Best Threshold: -1.0103, Variance Reduction: 4.515646\n",
            "          Feature 3 -> Best Threshold: -0.6644, Variance Reduction: 4.515646\n",
            "          Feature 4 -> Best Threshold: -1.0574, Variance Reduction: 4.515646\n",
            "          Feature 5 -> Best Threshold: 0.1035, Variance Reduction: 4.515646\n",
            "          Feature 6 -> Best Threshold: 1.0104, Variance Reduction: 4.515646\n",
            "          Feature 7 -> Best Threshold: -1.3478, Variance Reduction: 4.515646\n",
            "        Depth 4: Selected Feature 0 at Threshold -1.3829374754133272, Variance Reduction: 4.515646\n",
            "          Leaf node created at depth 5: Value = 5.0000\n",
            "          Leaf node created at depth 5: Value = 0.7500\n",
            "        Leaf node created at depth 4: Value = 0.9550\n",
            "\n",
            "Finished building the Regression Tree!\n",
            "\n",
            "\n",
            "Mean Squared Error on Test Set: 1.2838\n"
          ]
        }
      ]
    }
  ]
}
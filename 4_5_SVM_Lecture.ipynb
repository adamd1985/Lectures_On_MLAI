{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamd1985/Lectures_On_MLAI/blob/main/4_5_SVM_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines (SVM)\n",
        "\n",
        "In this notebook, we will implement the algorithms taught in the lesson on Support Vector Machines."
      ],
      "metadata": {
        "id": "Jf1iiLIFzdBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "8rw3utFn1d35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the Iris dataset"
      ],
      "metadata": {
        "id": "UtbCcW5f3CSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast_cancer = load_breast_cancer(as_frame=True)\n",
        "breast_cancer.frame.sample(3)"
      ],
      "metadata": {
        "id": "0ajzKtYm4o14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = breast_cancer.data.iloc[:, :2] # first 2 mean texture, mean radius.\n",
        "y = breast_cancer.target\n",
        "y = np.where(y == 0, -1, 1)  # Perceptron: -1 is not malignant.\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "KSFlHz_O3Gs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Perceptron Algorithm\n",
        "\n",
        "The Perceptron is a simple linear classifier that updates its weights based on misclassified examples. Given a dataset $\\mathcal{D} = \\{(x^{(n)}, y^{(n)})\\}$ where $y^{(n)} \\in \\{-1, +1\\}$ (sometimes used as $v$, induced local field), the model predicts:\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(n)} = \\text{sign}(\\beta^\\top x^{(n)} + \\beta_0)\n",
        "$$\n",
        "\n",
        "### Weight Update Rule\n",
        "\n",
        "If $\\hat{y}^{(n)} \\neq y^{(n)}$, the weights are updated as:\n",
        "\n",
        "$$\n",
        "\\beta \\leftarrow \\beta + \\eta y^{(n)} x^{(n)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\beta_0 \\leftarrow \\beta_0 + \\eta y^{(n)}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate.\n",
        "\n",
        "If $\\hat{y}^{(n)} = y^{(n)}$, no update is performed, meaning:\n",
        "\n",
        "$$\n",
        "\\beta \\leftarrow \\beta\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\beta_0 \\leftarrow \\beta_0\n",
        "$$\n",
        "\n",
        "### Convergence\n",
        "\n",
        "If the data is linearly separable, the perceptron is guaranteed to converge to a solution. Otherwise, it oscillates indefinitely.\n"
      ],
      "metadata": {
        "id": "rlvQkMkezaet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, max_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in tqdm(range(self.max_iters)):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                if y[idx] * (np.dot(x_i, self.w) + self.b) <= 0:\n",
        "                    self.w += self.lr * y[idx] * x_i\n",
        "                    self.b += self.lr * y[idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)"
      ],
      "metadata": {
        "id": "179kc6mW276l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "perceptron = Perceptron(learning_rate=0.01, max_iters=1000)\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_test)\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "KqQbfTMI96kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the decision boundaries between 2 features"
      ],
      "metadata": {
        "id": "6E7pS2nT4LY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(X, y, model, feature_indices=(0, 1), feature_names=(\"Feature 1\", \"Feature 2\")):\n",
        "    X_selected = X[:, feature_indices]\n",
        "\n",
        "    x_min, x_max = X_selected[:, 0].min() - 1, X_selected[:, 0].max() + 1\n",
        "    y_min, y_max = X_selected[:, 1].min() - 1, X_selected[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    X_grid = np.zeros((xx.ravel().shape[0], X.shape[1]))\n",
        "    X_grid[:, feature_indices[0]] = xx.ravel()\n",
        "    X_grid[:, feature_indices[1]] = yy.ravel()\n",
        "\n",
        "    Z = model.predict(X_grid)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, levels=np.linspace(Z.min(), Z.max(), 3))\n",
        "    plt.scatter(X_selected[:, 0], X_selected[:, 1], c=y, edgecolors='k', marker='o', label=\"Data Points\", alpha=0.6)\n",
        "\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "    plt.xlabel(feature_names[0])\n",
        "    plt.ylabel(feature_names[1])\n",
        "    plt.title(\"Decision Boundary\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_decision_boundary(X_test, y_test, model=perceptron, feature_names=breast_cancer.feature_names[:2])"
      ],
      "metadata": {
        "id": "5g5IlYo_4Fmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was easy, here the boundary is linearly serperable.\n",
        "\n",
        "What if we chose a set that are not?"
      ],
      "metadata": {
        "id": "H08u9nTX_OVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = breast_cancer.data.iloc[:, [0,3]]\n",
        "y = breast_cancer.target\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "pDCFJsAP_djN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron(learning_rate=0.01, max_iters=1000)\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_test)\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "kDHcAoym_Xys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An 83% accuracy with a linear decision boundary suggests that mean texture and mean area are not perfectly linearly separable.\n",
        "\n",
        "Many data points are very close to the decision boundary, and some misclassified points.\n",
        "\n",
        "Since the data points follow a curved pattern, a linear model cannot fully capture their relationship."
      ],
      "metadata": {
        "id": "XOTiMW7EEGaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(X_test, y_test, model=perceptron, feature_names=(breast_cancer.feature_names[1], breast_cancer.feature_names[3]))"
      ],
      "metadata": {
        "id": "QoXzzTpn_zVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVMs"
      ],
      "metadata": {
        "id": "E3TDWBgZ_FEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM"
      ],
      "metadata": {
        "id": "E39P14tL5hiR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkIwzMmaGKUQ"
      },
      "source": [
        "Support Vector Machines (SVMs) extend the perceptron by introducing the concept of maximizing margins. Instead of just finding a separating hyperplane, SVM optimizes for the decision boundary with the widest margin between classes. Given a dataset $\\mathcal{D} = \\{(x^{(n)}, y^{(n)})\\}$ with $y^{(n)} \\in \\{-1,+1\\}$, SVM solves:\n",
        "\n",
        "$$\n",
        "\\min_{\\beta, \\beta_0} \\frac{1}{2} ||\\beta||^2\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "y^{(n)} (\\beta^\\top x^{(n)} + \\beta_0) \\geq 1, \\quad \\forall n\n",
        "$$\n",
        "\n",
        "This ensures that all points lie outside the margin. The margin is given by:\n",
        "\n",
        "$$\n",
        "M = \\frac{2}{||\\beta||}\n",
        "$$\n",
        "\n",
        "To find the optimal decision boundary, we use **subgradient descent** to minimize the **hinge loss** with **L2 regularization**. The **hinge loss** is defined as:\n",
        "\n",
        "$$\n",
        "L_{\\text{hinge}} = \\sum_{n} \\max(0, 1 - y^{(n)} (\\beta^\\top x^{(n)} + \\beta_0)) + \\lambda ||\\beta||^2\n",
        "$$\n",
        "\n",
        "The **decision function** determines whether a sample satisfies the margin constraint:\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(n)} = \\text{sign}(\\beta^\\top x^{(n)} + \\beta_0)\n",
        "$$\n",
        "\n",
        "- If the current sample **satisfies** the margin constraint $ y^{(n)} \\hat{y}^{(n)} \\geq 1$:\n",
        "  $$\n",
        "  \\beta \\leftarrow \\beta - \\eta (2 \\lambda \\beta)\n",
        "  $$\n",
        "\n",
        "- If the current sample **violates** the margin constraint $ y^{(n)} \\hat{y}^{(n)} < 1$:\n",
        "  $$\n",
        "  \\beta \\leftarrow \\beta - \\eta (2 \\lambda \\beta - y^{(n)} x^{(n)})\n",
        "  $$\n",
        "  $$\n",
        "  \\beta_0 \\leftarrow \\beta_0 - \\eta y^{(n)}\n",
        "  $$\n",
        "\n",
        "where:\n",
        "- $ \\eta $ is the **learning rate**.\n",
        "- $ \\lambda $ is the **regularization parameter** controlling margin maximization.\n",
        "- $ \\beta $ is the **weight vector**, and $ \\beta_0 $ is the **bias**.\n",
        "- The **decision function** $ \\hat{y} = \\text{sign}(\\beta^\\top x + \\beta_0) $ is used to determine classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3MQLll4GKUS"
      },
      "source": [
        "class SupportVectorMachine:\n",
        "    def __init__(self, learning_rate=0.01, lambda_param=0.01, max_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.max_iters = max_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in tqdm(range(self.max_iters)):\n",
        "            margins = y * (X @ self.w + self.b)\n",
        "            misclassified = margins < 1\n",
        "\n",
        "            grad_w = 2 * self.lambda_param * self.w - np.mean((misclassified * y)[:, None] * X, axis=0)\n",
        "            grad_b = -np.mean(misclassified * y)\n",
        "\n",
        "            self.w -= self.lr * grad_w\n",
        "            self.b -= self.lr * grad_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(X @ self.w + self.b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = breast_cancer.data.iloc[:, [0,3]]\n",
        "y = breast_cancer.target\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SupportVectorMachine(learning_rate=0.01)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "xRwPAbAkJITS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(X_test, y_test, model=svm, feature_names=(breast_cancer.feature_names[1], breast_cancer.feature_names[3]))"
      ],
      "metadata": {
        "id": "3MThQ4m1JnWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft-Margin SVM\n",
        "\n",
        "Soft-margin SVM allows some misclassification by introducing slack variables $\\xi_i$, balancing margin maximization and error minimization. The optimization problem is:\n",
        "\n",
        "$$\n",
        "\\min_{w, b, \\xi} \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{n} \\xi_i\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "y_i (w^\\top x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i\n",
        "$$\n",
        "\n",
        "where $C$ controls the trade-off between maximizing the margin and minimizing classification.\n",
        "\n",
        "In gradient-based optimization, the weight update incorporates slack terms:\n",
        "\n",
        "$$\n",
        "w \\leftarrow w - \\eta \\left( 2\\lambda w - C \\sum_{i \\in \\mathcal{M}} y_i x_i \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "b \\leftarrow b - \\eta \\left( -C \\sum_{i \\in \\mathcal{M}} y_i \\right)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{M} = \\{i | y_i (w^\\top x_i + b) < 1 \\}$ are the misclassified points."
      ],
      "metadata": {
        "id": "qYtsQRuxq_dR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupportVectorMachineSlack:\n",
        "    def __init__(self, learning_rate=0.01, lambda_param=0.01, C=1.0, max_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.C = C\n",
        "        self.max_iters = max_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in tqdm(range(self.max_iters)):\n",
        "            margins = y * (X @ self.w + self.b)\n",
        "            slack = margins < 1\n",
        "\n",
        "            grad_w = 2 * self.lambda_param * self.w - self.C * np.mean((slack * y)[:, None] * X, axis=0)\n",
        "            grad_b = -self.C * np.mean(slack * y)\n",
        "\n",
        "            self.w -= self.lr * grad_w\n",
        "            self.b -= self.lr * grad_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(X @ self.w + self.b)\n"
      ],
      "metadata": {
        "id": "onBfg0T1rAXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = breast_cancer.data.iloc[:, [0,3]]\n",
        "y = breast_cancer.target\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SupportVectorMachineSlack(learning_rate=0.01, C=1.5)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "KgHkI5rlrmp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(X_test, y_test, model=svm, feature_names=(breast_cancer.feature_names[1], breast_cancer.feature_names[3]))"
      ],
      "metadata": {
        "id": "KsUug9RrrsBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dual Formulation of Soft-Margin SVM\n",
        "\n",
        "Support Vector Machines (SVM) optimize a **Lagrangian formulation** to maximize the margin while allowing some misclassification. The dual form is derived using **Lagrange multipliers**, introducing $\\alpha_i$ for each constraint:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "$$\n",
        "0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $K(x_i, x_j)$ is a general **kernel function**. For a **linear SVM**, $K(x_i, x_j) = x_i^\\top x_j$.\n",
        "- **Soft-margin SVM** introduces slack variables $\\xi_i$ in the primal formulation, but they are **absorbed** into the dual through the constraint $0 \\leq \\alpha_i \\leq C$.\n",
        "\n",
        "\n",
        "Once $\\alpha$ is optimized, the **weight vector** for a linear SVM is:\n",
        "$$\n",
        "w = \\sum_{i=1}^{n} \\alpha_i y_i x_i.\n",
        "$$\n",
        "\n",
        "The **bias term** is computed using **support vectors with $0 < \\alpha_i < C$**:\n",
        "$$\n",
        "b = y_i - \\sum_{j=1}^{n} \\alpha_j y_j K(x_j, x_i), \\quad \\text{for any } 0 < \\alpha_i < C.\n",
        "$$\n",
        "\n",
        "\n",
        "Instead of computing $w$ explicitly, predictions are made using the **kernel function**:\n",
        "$$\n",
        "f(x) = \\sum_{i=1}^{n} \\alpha_i y_i K(x_i, x) + b.\n",
        "$$"
      ],
      "metadata": {
        "id": "Xg4ArDPItdcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing Alphas in SVMs with SMO\n",
        "\n",
        "Support Vector Machines (SVMs) in their **dual formulation** require optimizing **Lagrange multipliers** ($\\alpha_i$) while maintaining constraints. To efficiently solve this, we use the **Sequential Minimal Optimization (SMO) algorithm**.\n",
        "\n",
        "Instead of optimizing **all** $\\alpha$ values at once (which is computationally expensive), SMO:\n",
        "- Selects **two $\\alpha$ values at a time** to update.\n",
        "- Ensures **the sum constraint**:\n",
        "  $$\n",
        "  \\sum_{i} \\alpha_i y_i = 0\n",
        "  $$\n",
        "- **Efficiently finds the optimal $\\alpha$ values** while keeping constraints valid.\n",
        "\n",
        "\n",
        "Step 1: Select Two Alphas ($\\alpha_i$ and $\\alpha_j$)\n",
        "Instead of updating **all** $\\alpha$ values at once, SMO picks **two indices ($i$ and $j$)** and updates only these two.\n",
        "\n",
        "- Compute **the error for $\\alpha_i$**:\n",
        "  $$\n",
        "  E_i = \\sum_j \\alpha_j y_j K(x_i, x_j) + b - y_i\n",
        "  $$\n",
        "- Select a **random second index $j$**.\n",
        "\n",
        "Step 2: Compute Bounds ($L$ and $H$)\n",
        "The values of $\\alpha_i$ and $\\alpha_j$ must **stay within a valid range**:\n",
        "\n",
        "- If $ y_i \\neq y_j $, the bounds are:\n",
        "  $$\n",
        "  L = \\max(0, \\alpha_j - \\alpha_i)\n",
        "  $$\n",
        "  \n",
        "  $$\n",
        "  H = \\min(C, C + \\alpha_j - \\alpha_i)\n",
        "  $$\n",
        "- If $ y_i = y_j $, the bounds are:\n",
        "  $$\n",
        "  L = \\max(0, \\alpha_i + \\alpha_j - C)\n",
        "  $$\n",
        "  $$\n",
        "  H = \\min(C, \\alpha_i + \\alpha_j)\n",
        "  $$\n",
        "\n",
        "If $ L = H $, no update is needed.\n",
        "\n",
        "Step 3: Compute $\\eta$ (Step Size)\n",
        "To determine the **step size** of updates, we compute:\n",
        "\n",
        "$$\n",
        "\\eta = 2K(x_i, x_j) - K(x_i, x_i) - K(x_j, x_j)\n",
        "$$\n",
        "\n",
        "If $ \\eta \\geq 0 $, we **skip the update** because it would increase the loss.\n",
        "\n",
        "Step 4: Update $\\alpha_j$ and Clip It\n",
        "Using the SMO formula:\n",
        "\n",
        "$$\n",
        "\\alpha_j = \\alpha_j - \\frac{y_j (E_i - E_j)}{\\eta}\n",
        "$$\n",
        "\n",
        "Then, **clip** $\\alpha_j$ to stay within the bounds:\n",
        "\n",
        "$$\n",
        "\\alpha_j = \\min(H, \\max(L, \\alpha_j))\n",
        "$$\n",
        "\n",
        "If the change in $\\alpha_j$ is too small, we stop updating.\n",
        "\n",
        "\n",
        "\n",
        "Step 5: Update $\\alpha_i$ to Maintain Constraints\n",
        "To ensure $ \\sum \\alpha_i y_i = 0 $, we update $\\alpha_i$:\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\alpha_i + y_i y_j (\\alpha_j^{\\text{old}} - \\alpha_j)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Step 6: Update the Bias ($b$)\n",
        "Since $\\alpha$ values changed, we must also **update the bias term**:\n",
        "\n",
        "$$\n",
        "b_1 = b - E_i - y_i (\\alpha_i - \\alpha_i^{\\text{old}}) K(x_i, x_i) - y_j (\\alpha_j - \\alpha_j^{\\text{old}}) K(x_i, x_j)\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_2 = b - E_j - y_i (\\alpha_i - \\alpha_i^{\\text{old}}) K(x_i, x_j) - y_j (\\alpha_j - \\alpha_j^{\\text{old}}) K(x_j, x_j)\n",
        "$$\n",
        "\n",
        "- If $ 0 < \\alpha_i < C $, set $ b = b_1 $.\n",
        "- Else if $ 0 < \\alpha_j < C $, set $ b = b_2 $.\n",
        "- Otherwise, set $ b = \\frac{b_1 + b_2}{2} $.\n",
        "\n",
        "Step 7: Repeat Until Convergence\n",
        "- Keep selecting new pairs $(i, j)$ and updating $\\alpha$ values.\n",
        "- Stop when the **change in $\\alpha$ is small** (converged)."
      ],
      "metadata": {
        "id": "xY4MNleVK9yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupportVectorMachineDual:\n",
        "    def __init__(self, C=1.0, kernel=None, max_iters=1000, tol=1e-5):\n",
        "        self.C = C\n",
        "        self.kernel = kernel if kernel else self.linear_kernel\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "        self.alpha = None\n",
        "        self.b = 0\n",
        "        self.support_vectors = None\n",
        "        self.support_labels = None\n",
        "        self.support_alphas = None\n",
        "\n",
        "    def linear_kernel(self, X1, X2):\n",
        "        return X1 @ X2.T\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = y.astype(float)\n",
        "        K = self.kernel(X, X)\n",
        "\n",
        "        self.alpha = np.zeros(n_samples)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in tqdm(range(self.max_iters)):\n",
        "            alpha_prev = np.copy(self.alpha)\n",
        "\n",
        "            for i in range(n_samples):\n",
        "                # Compute error for sample i\n",
        "                E_i = np.dot((self.alpha * y), K[:, i]) + self.b - y[i]\n",
        "                j = np.random.choice([x for x in range(n_samples) if x != i])\n",
        "                E_j = np.dot((self.alpha * y), K[:, j]) + self.b - y[j]\n",
        "\n",
        "                alpha_i_old, alpha_j_old = self.alpha[i], self.alpha[j]\n",
        "\n",
        "                # Compute L and H (bounds for alpha)\n",
        "                if y[i] != y[j]:\n",
        "                    L = max(0, self.alpha[j] - self.alpha[i])\n",
        "                    H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
        "                else:\n",
        "                    L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
        "                    H = min(self.C, self.alpha[i] + self.alpha[j])\n",
        "\n",
        "                if L == H:\n",
        "                    continue\n",
        "\n",
        "                # Compute second derivative of the objective function\n",
        "                eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
        "                if eta >= 0:\n",
        "                    continue\n",
        "\n",
        "                # Update alpha_j\n",
        "                self.alpha[j] -= (y[j] * (E_i - E_j)) / eta\n",
        "                self.alpha[j] = np.clip(self.alpha[j], L, H)\n",
        "\n",
        "                if abs(self.alpha[j] - alpha_j_old) < self.tol:\n",
        "                    continue\n",
        "\n",
        "                # Update alpha_i to maintain sum constraint\n",
        "                self.alpha[i] += y[i] * y[j] * (alpha_j_old - self.alpha[j])\n",
        "\n",
        "                # Compute b (bias term)\n",
        "                b1 = self.b - E_i - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] - y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]\n",
        "                b2 = self.b - E_j - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] - y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]\n",
        "\n",
        "                if 0 < self.alpha[i] < self.C:\n",
        "                    self.b = b1\n",
        "                elif 0 < self.alpha[j] < self.C:\n",
        "                    self.b = b2\n",
        "                else:\n",
        "                    self.b = (b1 + b2) / 2\n",
        "\n",
        "            diff = np.linalg.norm(self.alpha - alpha_prev)\n",
        "            if diff < self.tol:\n",
        "                break\n",
        "\n",
        "        support_vector_indices = self.alpha > 1e-6\n",
        "        self.support_vectors = X[support_vector_indices]\n",
        "        self.support_labels = y[support_vector_indices]\n",
        "        self.support_alphas = self.alpha[support_vector_indices]\n",
        "\n",
        "    def predict(self, X):\n",
        "        K_test = self.kernel(X, self.support_vectors)\n",
        "        return np.sign(np.sum(self.support_alphas * self.support_labels * K_test, axis=1) + self.b)"
      ],
      "metadata": {
        "id": "c6bIGW2gtfpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = breast_cancer.data.iloc[:, [0,3]]\n",
        "y = breast_cancer.target\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SupportVectorMachineDual(C=0.5)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "ZKbK-_wbuu4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(X_test, y_test, model=svm, feature_names=(breast_cancer.feature_names[1], breast_cancer.feature_names[3]))"
      ],
      "metadata": {
        "id": "wTp_RzueuyNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Kernel Trick in Support Vector Machines (SVM)\n",
        "\n",
        "Support Vector Machines (SVMs) can handle nonlinear classification problems using the **kernel trick**.\n",
        "\n",
        "\n",
        "- **Linear Kernel**: Preserves the original feature space.\n",
        "  \n",
        "  $$\n",
        "  K(x, x') = x^\\top x'\n",
        "  $$\n",
        "\n",
        "- **Polynomial Kernel**: Introduces interactions between features, enabling curved decision boundaries.\n",
        "\n",
        "  $$\n",
        "  K(x, x') = (x^\\top x' + c)^d\n",
        "  $$\n",
        "\n",
        "- **Radial Basis Function (RBF) Kernel**: Maps points into a higher-dimensional space using a Gaussian transformation, allowing flexible decision boundaries.\n",
        "\n",
        "  $$\n",
        "  K(x, x') = \\exp(-\\gamma ||x - x'||^2)\n",
        "  $$\n",
        "\n",
        "- **Sigmoid Kernel**: Resembles activation functions in neural networks.\n",
        "\n",
        "  $$\n",
        "  K(x, x') = \\tanh(\\alpha x^\\top x' + c)\n",
        "  $$\n",
        "\n",
        "Each kernel provides a different way of transforming the data while keeping computations in the dual space"
      ],
      "metadata": {
        "id": "1V8j0NO16IIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = breast_cancer.data.iloc[:, [0,1]]\n",
        "y = breast_cancer.target\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def rbf_kernel(X1, X2, gamma=0.5):\n",
        "    sq_dist = np.linalg.norm(X1[:, np.newaxis] - X2, axis=2) ** 2\n",
        "    return np.exp(-gamma * sq_dist)\n",
        "\n",
        "\n",
        "svm_rbf = SupportVectorMachineDual(C=0.8, kernel=lambda X1, X2: rbf_kernel(X1, X2, gamma=3.5))\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred = svm_rbf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "plot_decision_boundary(X_test, y_test, model=svm_rbf, feature_names=(breast_cancer.feature_names[1], breast_cancer.feature_names[3]))"
      ],
      "metadata": {
        "id": "H0WJl6JmwI4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "X = breast_cancer.data.iloc[:, [0, 1]]\n",
        "y = np.where(breast_cancer.target == 0, -1, 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def plot_svm_decision_boundary(kernel, X_train, y_train, X_test, y_test, ax, feature_names, class_names):\n",
        "    svc_clf = make_pipeline(StandardScaler(), SVC(kernel=kernel, C=1))\n",
        "    svc_clf.fit(X_train, y_train)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "\n",
        "    y_pred = svc_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    common_params = {\"estimator\": svc_clf, \"X\": X_train, \"ax\": ax}\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        **common_params,\n",
        "        response_method=\"predict\",\n",
        "        plot_method=\"pcolormesh\",\n",
        "        alpha=0.3,\n",
        "    )\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        **common_params,\n",
        "        response_method=\"decision_function\",\n",
        "        plot_method=\"contour\",\n",
        "        levels=[-1, 0, 1],\n",
        "        colors=[\"k\", \"k\", \"k\"],\n",
        "        linestyles=[\"--\", \"-\", \"--\"],\n",
        "    )\n",
        "\n",
        "    support_vectors = svc_clf.named_steps['standardscaler'].inverse_transform(\n",
        "        svc_clf.named_steps['svc'].support_vectors_\n",
        "    )\n",
        "    ax.scatter(\n",
        "        support_vectors[:, 0],\n",
        "        support_vectors[:, 1],\n",
        "        s=150,\n",
        "        facecolors=\"none\",\n",
        "        edgecolors=\"k\",\n",
        "        label=\"Support Vectors\",\n",
        "    )\n",
        "\n",
        "    scatter = ax.scatter(\n",
        "        X_train[:, 0],\n",
        "        X_train[:, 1],\n",
        "        c=y_train,\n",
        "        cmap=plt.cm.Paired,\n",
        "        edgecolors=\"k\",\n",
        "        s=50,\n",
        "    )\n",
        "\n",
        "    legend_labels = [class_names[int(label)] for label in np.unique(y_train)]\n",
        "    ax.legend(\n",
        "        scatter.legend_elements()[0],\n",
        "        legend_labels,\n",
        "        loc=\"upper right\",\n",
        "        title=\"Classes\",\n",
        "    )\n",
        "\n",
        "    ax.set_title(f\"{kernel} kernel in SVC\\nAccuracy: {accuracy:.2f}\\n({feature_names[0]} vs {feature_names[1]})\")\n",
        "\n",
        "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "for kernel, ax in zip(kernels, axes.ravel()):\n",
        "    plot_svm_decision_boundary(kernel, X_train, y_train, X_test, y_test, ax,\n",
        "                               feature_names=[breast_cancer.feature_names[0], breast_cancer.feature_names[1]],\n",
        "                               class_names=[\"Benign\", \"Malignant\"])\n"
      ],
      "metadata": {
        "id": "AcxhS4ar6Wm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Regression (SVR)\n",
        "\n",
        "Support Vector Regression (SVR) extends Support Vector Machines (SVM) to regression tasks. Instead of finding a maximum margin separator, SVR tries to fit a function $f(\\beta)$ that deviates from the true outputs by at most a margin $\\epsilon$, while minimizing model complexity. The optimization problem is formulated as:\n",
        "\n",
        "$$\n",
        "\\min_{\\beta, b, \\xi, \\xi^*} \\frac{1}{2} ||\\beta||^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "y_i - (\\beta^T \\beta_i + b) \\leq \\epsilon + \\xi_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "(\\beta^T \\beta_i + b) - y_i \\leq \\epsilon + \\xi_i^*\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\xi_i, \\xi_i^* \\geq 0\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\beta$ represents the weight vector,\n",
        "- $b$ is the bias term,\n",
        "- $\\xi_i, \\xi_i^*$ are slack variables that allow violations of the $\\epsilon$-tube,\n",
        "- $C$ controls the trade-off between model complexity and tolerance for outliers,\n",
        "- $\\epsilon$ defines the margin of tolerance."
      ],
      "metadata": {
        "id": "HGtTCbmpB45o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dual Formulation of SVR\n",
        "\n",
        "In the **dual formulation**, slack variables are not explicitly needed, as they are absorbed into the **Lagrange multipliers**. The **optimization problem** is formulated as:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha, \\alpha_j} \\sum_{i=1}^{n} ( \\alpha_j - \\alpha_i ) y_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} ( \\alpha_j - \\alpha_i )( \\alpha_j - \\alpha_i ) K(x_i, x_j)\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "0 \\leq \\alpha_i, \\alpha_j \\leq C, \\quad \\sum_{i=1}^{n} (\\alpha_i - \\alpha_j) = 0\n",
        "$$\n",
        "\n",
        "where:\n",
        "- **$\\alpha_i, \\alpha_j$** are **Lagrange multipliers**, which replace the slack variables.\n",
        "- **$ K(x_i, x_j) $** is the **kernel function**, allowing non-linear regression.\n",
        "- The constraint **$\\sum_{i} (\\alpha_i - \\alpha_j) = 0$** ensures balance between **positive and negative deviations**.\n",
        "\n",
        "Once the **optimal multipliers** $ \\alpha_i, \\alpha_j $ are found, the final regression function is:\n",
        "\n",
        "$$\n",
        "f(x) = \\sum_{i=1}^{n} (\\alpha_i - \\alpha_j) K(x_i, x) + b\n",
        "$$\n",
        "\n",
        "where $ b $ is the bias term computed from **support vectors**.\n"
      ],
      "metadata": {
        "id": "G64zK_wQZTw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "class LinearSVR:\n",
        "    def __init__(self, C=1.0, epsilon=0.1, learning_rate=0.01, max_iter=100):\n",
        "        self.C = C\n",
        "        self.epsilon = epsilon\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in tqdm(range(self.max_iter)):\n",
        "            y_pred = self.predict(X)\n",
        "            dw, db = self._compute_gradients(X, y, y_pred)\n",
        "\n",
        "            self.w -= self.learning_rate * dw\n",
        "            self.b -= self.learning_rate * db\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _compute_gradients(self, X, y, y_pred):\n",
        "        n_samples = X.shape[0]\n",
        "        dw = np.zeros_like(self.w)\n",
        "        db = 0\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            residual = y_pred[i] - y[i]\n",
        "            if abs(residual) <= self.epsilon:\n",
        "                continue\n",
        "\n",
        "            # epsilon-insensitive loss\n",
        "            if residual > 0:\n",
        "                dw += X[i]\n",
        "                db += 1\n",
        "            else:\n",
        "                dw -= X[i]\n",
        "                db -= 1\n",
        "        dw = dw / n_samples + self.C * self.w\n",
        "        db = db / n_samples\n",
        "\n",
        "        return dw, db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.w) + self.b\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data[:, :2]\n",
        "y = data.target\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "svr = LinearSVR(C=1.0, epsilon=0.1, learning_rate=0.01, max_iter=1000)\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = svr.predict(X_train)\n",
        "y_test_pred = svr.predict(X_test)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nSVR MSE: {test_mse:.4f}, R²: {test_r2:.4f}\")"
      ],
      "metadata": {
        "id": "qA-lVezHB6pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data[:2000, :1]\n",
        "y = data.target[:2000]\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "y = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def plot_svr_regression(kernel, X_train, y_train, X_test, y_test, ax, feature_name):\n",
        "    svr_clf = SVR(kernel=kernel, C=1.0, epsilon=0.1)\n",
        "    svr_clf.fit(X_train, y_train)\n",
        "\n",
        "    X_grid = np.linspace(X_train.min(), X_train.max(), 200).reshape(-1, 1)\n",
        "    y_pred_grid = svr_clf.predict(X_grid)\n",
        "    y_pred_test = svr_clf.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred_test)\n",
        "    r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "    ax.scatter(X_train, y_train, color=\"gray\", label=\"Data\")\n",
        "    ax.plot(X_grid, y_pred_grid, color=\"red\", label=f\"{kernel} SVR\")\n",
        "    ax.set_xlabel(feature_name)\n",
        "    ax.set_ylabel(\"Target\")\n",
        "    ax.set_title(f\"{kernel} SVR Regression\\nMSE: {mse:.2f}, R²: {r2:.2f}\")\n",
        "    ax.legend()\n",
        "\n",
        "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "for kernel, ax in zip(kernels, axes.ravel()):\n",
        "    plot_svr_regression(kernel, X_train, y_train, X_test, y_test, ax, feature_name=data.feature_names[0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mjH06pBUG73N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Class SVMs\n",
        "\n",
        "Multi class SMVs are a simple extension of the SMV algorithm, which instead of altering the models, it trains multiple models.\n",
        "\n",
        "We have OvR and OvO, for simplicity, we'll code the OvR variant."
      ],
      "metadata": {
        "id": "ythUx0_zSY6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "class OneVsRestSVM:\n",
        "    def __init__(self, C=1.0, kernel=None, max_iters=1000, tol=1e-5):\n",
        "        self.C = C\n",
        "        self.kernel = kernel\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "        self.classifiers = []\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            print(f\"Training SVM for class {self.classes[i]} vs rest\")\n",
        "            # 1 for current class, -1 for all others\n",
        "            binary_y = np.where(y == self.classes[i], 1, -1)\n",
        "            clf = SupportVectorMachineDual(\n",
        "                C=self.C,\n",
        "                kernel=self.kernel,\n",
        "                max_iters=self.max_iters,\n",
        "                tol=self.tol\n",
        "            )\n",
        "            clf.fit(X, binary_y)\n",
        "            self.classifiers.append(clf)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        decision_scores = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for i, clf in enumerate(self.classifiers):\n",
        "            K_test = clf.kernel(X, clf.support_vectors)\n",
        "            decision_scores[:, i] = np.sum(\n",
        "                clf.support_alphas * clf.support_labels * K_test,\n",
        "                axis=1\n",
        "            ) + clf.b\n",
        "\n",
        "        return self.classes[np.argmax(decision_scores, axis=1)]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes)\n",
        "        decision_scores = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        for i, clf in enumerate(self.classifiers):\n",
        "            K_test = clf.kernel(X, clf.support_vectors)\n",
        "            decision_scores[:, i] = np.sum(\n",
        "                clf.support_alphas * clf.support_labels * K_test,\n",
        "                axis=1\n",
        "            ) + clf.b\n",
        "        exp_scores = np.exp(decision_scores)\n",
        "        proba = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        return proba\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=100, random_state=42)\n",
        "svc_ovr = OneVsRestSVM(C=0.5)\n",
        "svc_ovr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svc_ovr.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nOne-vs-Rest SVC - Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "tvYPxzphStJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the decision boundaries"
      ],
      "metadata": {
        "id": "0YyVIJJnZSNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_multiclass_decision_boundary(X, y, model, feature_indices=(0, 1), feature_names=(\"Feature 1\", \"Feature 2\")):\n",
        "    X_selected = X[:, feature_indices]\n",
        "\n",
        "    x_min, x_max = X_selected[:, 0].min() - 1, X_selected[:, 0].max() + 1\n",
        "    y_min, y_max = X_selected[:, 1].min() - 1, X_selected[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    X_grid = np.zeros((xx.ravel().shape[0], X.shape[1]))\n",
        "    X_grid[:, feature_indices[0]] = xx.ravel()\n",
        "    X_grid[:, feature_indices[1]] = yy.ravel()\n",
        "\n",
        "    Z = model.predict(X_grid)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(len(np.unique(y)) + 1) - 0.5)\n",
        "    plt.scatter(X_selected[:, 0], X_selected[:, 1], c=y, edgecolors='k', marker='o', label=\"Data Points\")\n",
        "\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "    plt.xlabel(feature_names[0])\n",
        "    plt.ylabel(feature_names[1])\n",
        "    plt.title(\"Multiclass Decision Boundary (OvR SVC)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_multiclass_decision_boundary(X_test, y_test, model=svc_ovr, feature_names=data.feature_names[:2])"
      ],
      "metadata": {
        "id": "9LoNmrUrZN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with Scikit's"
      ],
      "metadata": {
        "id": "goblgmXeZWBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=100, random_state=42)\n",
        "\n",
        "svc_sklearn = SVC(C=0.5, kernel=\"linear\", decision_function_shape=\"ovr\")\n",
        "svc_sklearn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_sklearn = svc_sklearn.predict(X_test)\n",
        "\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"One-vs-Rest SVC - Accuracy: {accuracy_sklearn:.4f}\")"
      ],
      "metadata": {
        "id": "6xBSoTY7ZlB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "In this notebook, we explored Support Vector Machines (SVMs) for both classification and regression, along with their extension to multiclass classification using One-vs-Rest (OvR) and Support Vector Regression (SVR). We implemented linear, polynomial, radial basis function (RBF), and sigmoid kernels, analyzing their impact on decision boundaries and predictions.\n",
        "\n",
        "We have learned that:\n",
        "\n",
        "- The linear kernel is effective for linearly separable data and works well in both classification and regression.\n",
        "- The polynomial kernel introduces feature interactions, allowing for more flexible decision boundaries in multiclass classification.\n",
        "- The RBF kernel maps data into a high-dimensional space, making it powerful for capturing complex patterns in both SVM and SVR.\n",
        "- The sigmoid kernel, inspired by neural networks, can model non-traditional decision boundaries but is less commonly applied.\n",
        "- One-vs-Rest (OvR) extends binary SVMs to multiclass problems by training separate classifiers for each class.\n",
        "- Support Vector Regression (SVR) applies SVM principles to continuous target prediction, balancing margin control with model complexity."
      ],
      "metadata": {
        "id": "gwGPrbI9-9JV"
      }
    }
  ]
}
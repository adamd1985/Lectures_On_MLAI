{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamd1985/Lectures_On_MLAI/blob/main/4_7_Ensemble_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzxzQHkfrdYb"
      },
      "source": [
        "# Ensemble Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH8SrEHLrdYd"
      },
      "source": [
        "Ensemble methods combine the predictions of several other machine learning models, where by aggregating the results of the other models trained on the same dataset. These methods are classified into four general groups:\n",
        "\n",
        "- Voting Methods: make predictions based on the majority label predicted.\n",
        "  - Bagging Method: train individual models on random subsets, before voting.\n",
        "  - Boosting Methods: train individual models sequentially by learning from the errors before voting.\n",
        "- Stacking Methods: train individual models, and use a meta-model to aggregate their predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "twxZRdqzrdYd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the breast cancer dataset again in this notebook."
      ],
      "metadata": {
        "id": "ss0WNZIGgA1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast_cancer = load_breast_cancer(as_frame=True)\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "breast_cancer.frame.sample(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "aZnPXrdTgFFa",
        "outputId": "5fc4de6f-b9a8-45f3-de4c-b87ecae07992"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "379        11.08         18.83            73.3      361.6          0.12160   \n",
              "182        15.70         20.31           101.2      766.6          0.09597   \n",
              "417        15.50         21.08           102.9      803.1          0.11200   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "379           0.21540         0.16890              0.06367         0.2196   \n",
              "182           0.08799         0.06593              0.05189         0.1618   \n",
              "417           0.15710         0.15220              0.08481         0.2085   \n",
              "\n",
              "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
              "379                 0.07950  ...          32.82            91.76       508.1   \n",
              "182                 0.05549  ...          32.82           129.30      1269.0   \n",
              "417                 0.06864  ...          27.65           157.10      1748.0   \n",
              "\n",
              "     worst smoothness  worst compactness  worst concavity  \\\n",
              "379            0.2184             0.9379           0.8402   \n",
              "182            0.1414             0.3547           0.2902   \n",
              "417            0.1517             0.4002           0.4211   \n",
              "\n",
              "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
              "379                0.2524          0.4154                  0.14030       0  \n",
              "182                0.1541          0.3437                  0.08631       0  \n",
              "417                0.2134          0.3003                  0.10480       0  \n",
              "\n",
              "[3 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79cf8ccd-5270-4ec8-a8e0-498a358dbe74\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>11.08</td>\n",
              "      <td>18.83</td>\n",
              "      <td>73.3</td>\n",
              "      <td>361.6</td>\n",
              "      <td>0.12160</td>\n",
              "      <td>0.21540</td>\n",
              "      <td>0.16890</td>\n",
              "      <td>0.06367</td>\n",
              "      <td>0.2196</td>\n",
              "      <td>0.07950</td>\n",
              "      <td>...</td>\n",
              "      <td>32.82</td>\n",
              "      <td>91.76</td>\n",
              "      <td>508.1</td>\n",
              "      <td>0.2184</td>\n",
              "      <td>0.9379</td>\n",
              "      <td>0.8402</td>\n",
              "      <td>0.2524</td>\n",
              "      <td>0.4154</td>\n",
              "      <td>0.14030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>15.70</td>\n",
              "      <td>20.31</td>\n",
              "      <td>101.2</td>\n",
              "      <td>766.6</td>\n",
              "      <td>0.09597</td>\n",
              "      <td>0.08799</td>\n",
              "      <td>0.06593</td>\n",
              "      <td>0.05189</td>\n",
              "      <td>0.1618</td>\n",
              "      <td>0.05549</td>\n",
              "      <td>...</td>\n",
              "      <td>32.82</td>\n",
              "      <td>129.30</td>\n",
              "      <td>1269.0</td>\n",
              "      <td>0.1414</td>\n",
              "      <td>0.3547</td>\n",
              "      <td>0.2902</td>\n",
              "      <td>0.1541</td>\n",
              "      <td>0.3437</td>\n",
              "      <td>0.08631</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>15.50</td>\n",
              "      <td>21.08</td>\n",
              "      <td>102.9</td>\n",
              "      <td>803.1</td>\n",
              "      <td>0.11200</td>\n",
              "      <td>0.15710</td>\n",
              "      <td>0.15220</td>\n",
              "      <td>0.08481</td>\n",
              "      <td>0.2085</td>\n",
              "      <td>0.06864</td>\n",
              "      <td>...</td>\n",
              "      <td>27.65</td>\n",
              "      <td>157.10</td>\n",
              "      <td>1748.0</td>\n",
              "      <td>0.1517</td>\n",
              "      <td>0.4002</td>\n",
              "      <td>0.4211</td>\n",
              "      <td>0.2134</td>\n",
              "      <td>0.3003</td>\n",
              "      <td>0.10480</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79cf8ccd-5270-4ec8-a8e0-498a358dbe74')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79cf8ccd-5270-4ec8-a8e0-498a358dbe74 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79cf8ccd-5270-4ec8-a8e0-498a358dbe74');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8472056e-8c43-4ab0-8bc1-47df3eacd8f2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8472056e-8c43-4ab0-8bc1-47df3eacd8f2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8472056e-8c43-4ab0-8bc1-47df3eacd8f2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = breast_cancer.data.values, breast_cancer.target.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zX79hmzugvz3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nvWm4rDrdYg"
      },
      "source": [
        "# Voting Methods in Ensemble Learning\n",
        "\n",
        "Ensemble learning leverages multiple models to improve predictive performance. A fundamental technique in ensemble learning is **voting**, where multiple classifiers are trained independently, and their predictions are aggregated to make a final decision.\n",
        "\n",
        "Hard voting, also known as majority voting, involves taking the most frequently predicted class label among all base models. Given $m$ classifiers, each producing a prediction $y_i^j$ for sample $x_i$, the ensemble prediction is determined as:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\text{mode}(y_i^1, y_i^2, ..., y_i^m)\n",
        "$$\n",
        "\n",
        "where $\\text{mode}(\\cdot)$ returns the most common class among all base classifiers.\n",
        "\n",
        "For binary classification, this can be formally expressed using the indicator function:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\mathbb{I} \\left( \\frac{1}{m} \\sum_{j=1}^{m} y_i^j > 0.5 \\right)\n",
        "$$\n",
        "\n",
        "where $\\mathbb{I}$ is the indicator function that assigns class 1 if the majority votes for it; otherwise, class 0.\n",
        "\n",
        "\n",
        "Soft voting takes into account the predicted probabilities from each classifier rather than just the final class label. Each classifier outputs a probability distribution over the possible classes:\n",
        "\n",
        "$$\n",
        "P(y_i = c | x_i) = \\frac{1}{m} \\sum_{j=1}^{m} P_j(y_i = c | x_i)\n",
        "$$\n",
        "\n",
        "where $P_j(y_i = c | x_i)$ is the probability assigned to class $c$ by the $j$-th model.\n",
        "\n",
        "The final prediction is then assigned to the class with the highest averaged probability:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\arg\\max_{c} \\left( \\frac{1}{m} \\sum_{j=1}^{m} P_j(y_i = c | x_i) \\right)\n",
        "$$\n",
        "\n",
        "Soft voting generally works better when classifiers output well-calibrated probabilities.\n",
        "\n",
        "For regression tasks, the final prediction is computed as the average of all base model predictions:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{1}{m} \\sum_{j=1}^{m} \\hat{y}_i^j\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r2lIX-kArdYg",
        "outputId": "f0e60986-d3b6-4801-ba2c-8518bd568369",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy is 95.6140 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "lr_preds = lr_model.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_preds)\n",
        "print('Logistic Regression accuracy is {0:7.4f} %'.format(lr_acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QbNSEb_TrdYg",
        "outputId": "0c4bd3af-8877-485f-f447-99d2c3ebb56c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accuracy is 94.7368 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Set probability=True to access probas.\n",
        "svm_model = SVC(probability=True, random_state=1)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_acc = accuracy_score(y_test, svm_preds)\n",
        "print('SVM accuracy is {0:7.4f} %'.format(svm_acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "APLu1dXJrdYg",
        "outputId": "268e418a-e91a-4746-a86b-59b978a54429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree accuracy is 93.8596 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_model = DecisionTreeClassifier(random_state=1)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_preds = tree_model.predict(X_test)\n",
        "\n",
        "tree_acc = accuracy_score(y_test, tree_preds)\n",
        "print('Decision Tree accuracy is {0:7.4f} %'.format(tree_acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's build a hard and soft voting mechanism."
      ],
      "metadata": {
        "id": "-8zhJwThiVij"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZBuolfMdrdYg",
        "outputId": "29ee956a-90cb-481d-a7ef-b52a38df0dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Voting Classifier accuracy: 96.4912%\n",
            "Soft Voting Classifier accuracy: 95.6140%\n"
          ]
        }
      ],
      "source": [
        "# hard voting (majority vote)\n",
        "predictions = np.array([lr_preds, svm_preds, tree_preds])\n",
        "ensemble_hard_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
        "\n",
        "# soft voting (average probability)\n",
        "log_reg_probs = lr_model.predict_proba(X_test)\n",
        "svc_probs = svm_model.predict_proba(X_test)\n",
        "tree_probs = tree_model.predict_proba(X_test)\n",
        "avg_probs = (log_reg_probs + svc_probs + tree_probs) / 3\n",
        "ensemble_soft_preds = np.argmax(avg_probs, axis=1)\n",
        "\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Hard Voting Classifier accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Soft Voting Classifier accuracy: {soft_acc * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can validate using Scikit learn [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)."
      ],
      "metadata": {
        "id": "DHlu7n5QhoOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "hard_voting_clf = VotingClassifier(estimators=[('lr', lr_model), ('svc', svm_model), ('tree', tree_model)], voting='hard')\n",
        "hard_voting_clf.fit(X_train, y_train)\n",
        "ensemble_hard_preds = hard_voting_clf.predict(X_test)\n",
        "\n",
        "soft_voting_clf = VotingClassifier(estimators=[('lr', lr_model), ('svc', svm_model), ('tree', tree_model)], voting='soft')\n",
        "soft_voting_clf.fit(X_train, y_train)\n",
        "ensemble_soft_preds = soft_voting_clf.predict(X_test)\n",
        "\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Hard Voting Classifier accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Soft Voting Classifier accuracy: {soft_acc * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJjSg-3nj2iv",
        "outputId": "b76ba3b3-eb25-453b-ab60-4493fb2b3f2a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Voting Classifier accuracy: 96.4912%\n",
            "Soft Voting Classifier accuracy: 95.6140%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqrDCNoMrdYk"
      },
      "source": [
        "## Bagging Ensemble\n",
        "\n",
        "The term **bagging** is short for *bootstrap aggregating*, where bootstrapping refers to sampling subsets from the training data with replacement. Bagging is an ensemble learning technique that aims to reduce variance and improve model stability by training multiple base learners on different random subsets of the dataset and aggregating their predictions.\n",
        "\n",
        "Given a training dataset of size $N$, bagging creates $m$ bootstrap samples $D_1, D_2, ..., D_m$ by randomly sampling with replacement from the original dataset.\n",
        "\n",
        "Each base model $f_j(x)$ is trained independently on a bootstrap sample $D_j$ and produces a prediction $\\hat{y}_i^j$ for an input sample $x_i$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I1V3rJ-brdYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "n_estimators = 10\n",
        "tree_models = []\n",
        "for _ in range(n_estimators):\n",
        "    X_resampled, y_resampled = resample(X_train, y_train, random_state=1)\n",
        "    tree = DecisionTreeClassifier(random_state=1)\n",
        "    tree.fit(X_resampled, y_resampled)\n",
        "    tree_models.append(tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6wsDTsx-rdYk",
        "outputId": "96dd8cf6-aa35-4b54-ebe7-8835956aa89f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Hard Voting Classifier accuracy: 92.9825%\n",
            "Bagging Soft Voting Classifier accuracy: 92.9825%\n"
          ]
        }
      ],
      "source": [
        "tree_preds = np.array([tree.predict(X_test) for tree in tree_models])\n",
        "ensemble_hard_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_preds)\n",
        "\n",
        "tree_probs = np.mean([tree.predict_proba(X_test) for tree in tree_models], axis=0)\n",
        "ensemble_soft_preds = np.argmax(tree_probs, axis=1)\n",
        "\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Bagging Hard Voting Classifier accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Bagging Soft Voting Classifier accuracy: {soft_acc * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG1iglaQrdYl"
      },
      "source": [
        "Bagging Ensembles often outperform Voting Ensembles, and they can decrease the overfitting of Decision Trees (which tend to overfit easily). As well as, Bagging Ensembles can be trained in parallel using different CPU cores, which can reduce the processing time.  \n",
        "\n",
        "Random Forest can be considered a Bagging Ensemble trained on the whole dataset if you set max samples to 1, in fact let's check using scikit [RandomForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=10, max_samples=1.0, bootstrap=True, random_state=1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "ensemble_hard_preds = rf_model.predict(X_test)\n",
        "ensemble_soft_preds = np.argmax(rf_model.predict_proba(X_test), axis=1)\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Random Forest (Bagging) Hard Voting accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Random Forest (Bagging) Soft Voting accuracy: {soft_acc * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4SUPcqDmC7S",
        "outputId": "85286a8f-2fd0-44ba-a220-e535ef10af4b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (Bagging) Hard Voting accuracy: 95.6140%\n",
            "Random Forest (Bagging) Soft Voting accuracy: 95.6140%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrc0HpHNrdYl"
      },
      "source": [
        "## Boosting Ensemble\n",
        "\n",
        "Boosting is an ensemble learning technique that trains individual models **sequentially**, where each subsequent model learns from the errors of the previous models and attempts to improve overall performance. Unlike bagging, which trains models independently in parallel, boosting adjusts model weights dynamically to focus on **misclassified samples**.\n",
        "\n",
        "Given a dataset with $N$ training samples, boosting maintains a weight distribution $w_i$ for each training sample, adjusting it iteratively to give more importance to **misclassified samples**.\n",
        "\n",
        "Each model $f_j(x)$ is trained sequentially on a weighted version of the dataset $D$, where misclassified instances receive higher weights. The weight update is defined as:\n",
        "\n",
        "$$\n",
        "w_i^{(t+1)} = w_i^{(t)} e^{-\\alpha_t y_i f_t(x_i)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $w_i^{(t)}$ is the weight of sample $i$ at iteration $t$.\n",
        "- $\\alpha_t$ is the model weight, computed as:\n",
        "\n",
        "  $$\n",
        "  \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - e_t}{e_t} \\right)\n",
        "  $$\n",
        "\n",
        "  where $e_t$ is the weighted error rate of the weak learner.\n",
        "- $y_i$ is the true class label.\n",
        "- $f_t(x_i)$ is the prediction of the weak learner at step $t$.\n",
        "\n",
        "After training multiple weak models, the final ensemble prediction is obtained by weighted voting:\n",
        "\n",
        "$$\n",
        "F(x) = \\sum_{t=1}^{m} \\alpha_t f_t(x)\n",
        "$$\n",
        "\n",
        "For classification, the predicted label is:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{sign}(F(x))\n",
        "$$\n",
        "\n",
        "For regression, the final prediction is a weighted sum of weak learner outputs:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sum_{t=1}^{m} \\alpha_t f_t(x)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK52Ix40rdYl"
      },
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "Gradient Boosting is an ensemble learning technique that optimizes predictions by training models sequentially, where each model corrects the errors (residuals) of the previous model. Unlike other boosting methods, Gradient Boosting uses **gradient descent** to minimize a specific loss function, making it particularly effective for both regression and classification tasks.\n",
        "\n",
        "  The first model $F_0(x)$ is trained to predict the initial estimate, often using the mean value (for regression) or log-odds (for classification). For example, for regression:\n",
        "\n",
        "  $$\n",
        "  F_0(x) = \\arg\\min_c \\sum_{i=1}^N L(y_i, c)\n",
        "  $$\n",
        "\n",
        "  where $L(y_i, c)$ is the loss function, such as squared error.\n",
        "\n",
        "  For each subsequent step $m$, the model computes the **pseudo-residuals** (negative gradient of the loss function):\n",
        "\n",
        "  $$\n",
        "  r_i^{(m)} = - \\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}\n",
        "  $$\n",
        "\n",
        "  These residuals represent how much the current model’s predictions differ from the actual targets.\n",
        "\n",
        "  A weak learner $h_m(x)$ (usually a decision tree) is then trained to predict these residuals:\n",
        "\n",
        "  $$\n",
        "  h_m(x) = \\arg\\min_h \\sum_{i=1}^N \\left( r_i^{(m)} - h(x_i) \\right)^2\n",
        "  $$\n",
        "\n",
        "\n",
        "  The ensemble prediction is updated by adding the scaled weak learner’s predictions:\n",
        "\n",
        "  $$\n",
        "  F_m(x) = F_{m-1}(x) + \\eta h_m(x)\n",
        "  $$\n",
        "\n",
        "  where $\\eta$ is the learning rate that controls how much the weak learner contributes to the final model.\n",
        "\n",
        "\n",
        "  After $M$ steps, the final model is:\n",
        "\n",
        "  $$\n",
        "  F(x) = F_0(x) + \\sum_{m=1}^M \\eta h_m(x)\n",
        "  $$\n",
        "\n",
        "  For regression, this is the final predicted value. For classification, the final predictions are converted to probabilities, and the class with the highest probability is chosen.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "\n",
        "n_estimators = 50\n",
        "learning_rate = 0.1\n",
        "max_depth = 3\n",
        "\n",
        "# Initialize predictions with log odds\n",
        "p = np.mean(y_train)\n",
        "F_train = np.full_like(y_train, np.log(p / (1 - p)), dtype=np.float64)\n",
        "F_test = np.full_like(y_test, np.log(p / (1 - p)), dtype=np.float64)\n",
        "\n",
        "for _ in range(n_estimators):\n",
        "    # Compute pseudo residuals\n",
        "    p_train = 1 / (1 + np.exp(-F_train))\n",
        "    residuals = y_train - p_train\n",
        "\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "    tree.fit(X_train, residuals)\n",
        "\n",
        "    tree_preds = tree.predict(X_train)\n",
        "    numerator = np.sum(residuals * tree_preds)\n",
        "    denominator = np.sum(tree_preds * tree_preds)\n",
        "    gamma = numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "    F_train += learning_rate * gamma * tree.predict(X_train)\n",
        "    F_test += learning_rate * gamma * tree.predict(X_test)\n",
        "ensemble_probs = 1 / (1 + np.exp(-F_test))  # Convert log-odds to probability\n",
        "ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
        "\n",
        "logloss = log_loss(y_test, ensemble_probs)\n",
        "accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f'Gradient Boosting Log Loss: {logloss:.4f}')\n",
        "print(f'Gradient Boosting Accuracy: {accuracy * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H59fP1Iqn6ad",
        "outputId": "31ca8b0c-aa79-4ba7-b19f-b09707ef06df"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Log Loss: 0.2599\n",
            "Gradient Boosting Accuracy: 95.6140%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it against Scikit's [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
      ],
      "metadata": {
        "id": "QUvexYXaoQQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ovarxTzirdYl",
        "outputId": "2e9ee6b7-0288-4aeb-ccd2-26cb3c4ead92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Log Loss: 0.1121\n",
            "Gradient Boosting Accuracy: 95.6140%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "boosting_model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=1)\n",
        "boosting_model.fit(X_train, y_train)\n",
        "\n",
        "ensemble_probs = boosting_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "ensemble_preds = boosting_model.predict(X_test)\n",
        "logloss = log_loss(y_test, ensemble_probs)\n",
        "accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f'Gradient Boosting Log Loss: {logloss:.4f}')\n",
        "print(f'Gradient Boosting Accuracy: {accuracy * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZacirDzfrdYn"
      },
      "source": [
        "# Stacking Methods\n",
        "\n",
        "Stacking is an ensemble learning technique that combines multiple base models by training a higher-level **meta-model** to aggregate their predictions. Unlike hard or soft voting methods, stacking learns how to best combine base model outputs by leveraging another model that specializes in integrating predictions.\n",
        "\n",
        "Let $f_1(x), f_2(x), ..., f_m(x)$ represent the $m$ base models. For a given input $x$, the predictions from the base models are:\n",
        "\n",
        "$$\n",
        "\\mathbf{h}(x) = [f_1(x), f_2(x), ..., f_m(x)]\n",
        "$$\n",
        "\n",
        "These predictions form a new feature vector $\\mathbf{h}(x)$ for the meta-model $g(\\mathbf{h}(x))$. The final stacked prediction is then:\n",
        "\n",
        "$$\n",
        "\\hat{y} = g(\\mathbf{h}(x))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg = LogisticRegression(max_iter=10000, random_state=1)\n",
        "log_reg.fit(X_train, y_train)\n",
        "log_reg_preds = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "svc_model = SVC(probability=True, random_state=1)\n",
        "svc_model.fit(X_train, y_train)\n",
        "svc_preds = svc_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "tree_model = DecisionTreeClassifier(random_state=1)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_preds = tree_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "stacked_features = np.column_stack((log_reg_preds, svc_preds, tree_preds))\n",
        "\n",
        "meta_model = LogisticRegression(max_iter=10000, random_state=1)\n",
        "meta_model.fit(stacked_features, y_test)\n",
        "meta_preds = meta_model.predict_proba(stacked_features)[:, 1]\n",
        "final_preds = meta_model.predict(stacked_features)\n",
        "\n",
        "logloss = log_loss(y_test, meta_preds)\n",
        "accuracy = accuracy_score(y_test, final_preds)\n",
        "print(f'Stacking Log Loss: {logloss:.4f}')\n",
        "print(f'Stacking Accuracy: {accuracy * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuyzBibrrYt4",
        "outputId": "6e1491f2-9617-4af6-d009-0e5a95eae95c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Log Loss: 0.1162\n",
            "Stacking Accuracy: 96.4912%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "h-NPPn8VrdYn",
        "outputId": "8202a1ba-0cf7-4100-f611-5ec0a4ea9a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Log Loss: 0.0985\n",
            "Stacking Accuracy: 95.6140%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('log_reg', LogisticRegression(max_iter=10000, random_state=1)),\n",
        "    ('svc', SVC(probability=True, random_state=1)),\n",
        "    ('tree', DecisionTreeClassifier(random_state=1))\n",
        "]\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=10000, random_state=1))\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "ensemble_probs = stacking_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "ensemble_preds = stacking_model.predict(X_test)\n",
        "logloss = log_loss(y_test, ensemble_probs)\n",
        "accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f'Stacking Log Loss: {logloss:.4f}')\n",
        "print(f'Stacking Accuracy: {accuracy * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLvFnn_ArdYn"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we have explored several key ensemble learning techniques and their unique strengths:\n",
        "\n",
        "1. **Bagging:** By training multiple models on different bootstrapped subsets of the data, we learned how to reduce variance and create a robust ensemble that outperforms individual models. The Random Forest is a classic example that demonstrates bagging’s power to handle noise and provide stable predictions.\n",
        "\n",
        "2. **Boosting:** We saw how boosting sequentially trains weak learners, focusing on correcting the errors of prior models. By optimizing the loss function step-by-step, boosting algorithms—such as AdaBoost and Gradient Boosting—achieve high accuracy and are especially effective at handling complex patterns and imbalanced datasets.\n",
        "\n",
        "3. **Voting:** Hard and soft voting methods taught us how simple aggregations of multiple models’ predictions can yield improved performance. These approaches illustrate that even without sophisticated training strategies, combining diverse model types can enhance overall accuracy.\n",
        "\n",
        "4. **Stacking:** Finally, stacking introduced the concept of training a meta-model to learn how best to combine the outputs of multiple base models. By using a higher-level model to integrate predictions, we gain the ability to exploit the complementary strengths of various learning algorithms.\n",
        "\n",
        "We have seen that ensemble methods are some of the most effective tools in the machine learning toolbox. They consistently deliver top-tier performance on tabular data, often with minimal hyperparameter tuning. By understanding and leveraging these techniques, we can tackle a wide range of predictive tasks with confidence and reliability.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
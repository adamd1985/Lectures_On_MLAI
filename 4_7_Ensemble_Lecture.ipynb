{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamd1985/Lectures_On_MLAI/blob/main/4_7_Ensemble_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzxzQHkfrdYb"
      },
      "source": [
        "# Ensemble Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH8SrEHLrdYd"
      },
      "source": [
        "Ensemble methods combine the predictions of several other machine learning models, where by aggregating the results of the other models trained on the same dataset. These methods are classified into four general groups:\n",
        "\n",
        "- Voting Methods: make predictions based on the majority label predicted.\n",
        "  - Bagging Method: train individual models on random subsets, before voting.\n",
        "  - Boosting Methods: train individual models sequentially by learning from the errors before voting.\n",
        "- Stacking Methods: train individual models, and use a meta-model to aggregate their predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twxZRdqzrdYd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the breast cancer dataset again in this notebook."
      ],
      "metadata": {
        "id": "ss0WNZIGgA1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast_cancer = load_breast_cancer(as_frame=True)\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "breast_cancer.frame.sample(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "aZnPXrdTgFFa",
        "outputId": "509002d8-eab4-4ad9-ddba-30716458a7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "435        13.98         19.62           91.12      599.5          0.10600   \n",
              "372        21.37         15.10          141.30     1386.0          0.10010   \n",
              "376        10.57         20.22           70.15      338.3          0.09073   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "435            0.1133          0.1126              0.06463         0.1669   \n",
              "372            0.1515          0.1932              0.12550         0.1973   \n",
              "376            0.1660          0.2280              0.05941         0.2188   \n",
              "\n",
              "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
              "435                 0.06544  ...          30.80           113.90       869.3   \n",
              "372                 0.06183  ...          21.84           152.10      1535.0   \n",
              "376                 0.08450  ...          22.82            76.51       351.9   \n",
              "\n",
              "     worst smoothness  worst compactness  worst concavity  \\\n",
              "435            0.1613             0.3568           0.4069   \n",
              "372            0.1192             0.2840           0.4024   \n",
              "376            0.1143             0.3619           0.6030   \n",
              "\n",
              "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
              "435                0.1827          0.3179                  0.10550       0  \n",
              "372                0.1966          0.2730                  0.08666       0  \n",
              "376                0.1465          0.2597                  0.12000       1  \n",
              "\n",
              "[3 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88b183e5-984d-4d26-9396-efd0ea82f72f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>13.98</td>\n",
              "      <td>19.62</td>\n",
              "      <td>91.12</td>\n",
              "      <td>599.5</td>\n",
              "      <td>0.10600</td>\n",
              "      <td>0.1133</td>\n",
              "      <td>0.1126</td>\n",
              "      <td>0.06463</td>\n",
              "      <td>0.1669</td>\n",
              "      <td>0.06544</td>\n",
              "      <td>...</td>\n",
              "      <td>30.80</td>\n",
              "      <td>113.90</td>\n",
              "      <td>869.3</td>\n",
              "      <td>0.1613</td>\n",
              "      <td>0.3568</td>\n",
              "      <td>0.4069</td>\n",
              "      <td>0.1827</td>\n",
              "      <td>0.3179</td>\n",
              "      <td>0.10550</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>21.37</td>\n",
              "      <td>15.10</td>\n",
              "      <td>141.30</td>\n",
              "      <td>1386.0</td>\n",
              "      <td>0.10010</td>\n",
              "      <td>0.1515</td>\n",
              "      <td>0.1932</td>\n",
              "      <td>0.12550</td>\n",
              "      <td>0.1973</td>\n",
              "      <td>0.06183</td>\n",
              "      <td>...</td>\n",
              "      <td>21.84</td>\n",
              "      <td>152.10</td>\n",
              "      <td>1535.0</td>\n",
              "      <td>0.1192</td>\n",
              "      <td>0.2840</td>\n",
              "      <td>0.4024</td>\n",
              "      <td>0.1966</td>\n",
              "      <td>0.2730</td>\n",
              "      <td>0.08666</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>10.57</td>\n",
              "      <td>20.22</td>\n",
              "      <td>70.15</td>\n",
              "      <td>338.3</td>\n",
              "      <td>0.09073</td>\n",
              "      <td>0.1660</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.05941</td>\n",
              "      <td>0.2188</td>\n",
              "      <td>0.08450</td>\n",
              "      <td>...</td>\n",
              "      <td>22.82</td>\n",
              "      <td>76.51</td>\n",
              "      <td>351.9</td>\n",
              "      <td>0.1143</td>\n",
              "      <td>0.3619</td>\n",
              "      <td>0.6030</td>\n",
              "      <td>0.1465</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.12000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88b183e5-984d-4d26-9396-efd0ea82f72f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88b183e5-984d-4d26-9396-efd0ea82f72f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88b183e5-984d-4d26-9396-efd0ea82f72f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7998caf8-1a61-419b-bcf2-c881a34e4559\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7998caf8-1a61-419b-bcf2-c881a34e4559')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7998caf8-1a61-419b-bcf2-c881a34e4559 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = breast_cancer.data.values, breast_cancer.target.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zX79hmzugvz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nvWm4rDrdYg"
      },
      "source": [
        "# Voting Methods in Ensemble Learning\n",
        "\n",
        "Ensemble learning leverages multiple models to improve predictive performance. A fundamental technique in ensemble learning is **voting**, where multiple classifiers are trained independently, and their predictions are aggregated to make a final decision.\n",
        "\n",
        "Hard voting, also known as majority voting, involves taking the most frequently predicted class label among all base models. Given $m$ classifiers, each producing a prediction $y_i^j$ for sample $x_i$, the ensemble prediction is determined as:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\text{mode}(y_i^1, y_i^2, ..., y_i^m)\n",
        "$$\n",
        "\n",
        "where $\\text{mode}(\\cdot)$ returns the most common class among all base classifiers.\n",
        "\n",
        "For binary classification, this can be formally expressed using the indicator function:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\mathbb{I} \\left( \\frac{1}{m} \\sum_{j=1}^{m} y_i^j > 0.5 \\right)\n",
        "$$\n",
        "\n",
        "where $\\mathbb{I}$ is the indicator function that assigns class 1 if the majority votes for it; otherwise, class 0.\n",
        "\n",
        "\n",
        "Soft voting takes into account the predicted probabilities from each classifier rather than just the final class label. Each classifier outputs a probability distribution over the possible classes:\n",
        "\n",
        "$$\n",
        "P(y_i = c | x_i) = \\frac{1}{m} \\sum_{j=1}^{m} P_j(y_i = c | x_i)\n",
        "$$\n",
        "\n",
        "where $P_j(y_i = c | x_i)$ is the probability assigned to class $c$ by the $j$-th model.\n",
        "\n",
        "The final prediction is then assigned to the class with the highest averaged probability:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\arg\\max_{c} \\left( \\frac{1}{m} \\sum_{j=1}^{m} P_j(y_i = c | x_i) \\right)\n",
        "$$\n",
        "\n",
        "Soft voting generally works better when classifiers output well-calibrated probabilities.\n",
        "\n",
        "For regression tasks, the final prediction is computed as the average of all base model predictions:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{1}{m} \\sum_{j=1}^{m} \\hat{y}_i^j\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2lIX-kArdYg",
        "outputId": "0be894be-4a40-4fbc-a7e1-c6bf1992cb3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression accuracy is 95.6140 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "lr_preds = lr_model.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_preds)\n",
        "print('Logistic Regression accuracy is {0:7.4f} %'.format(lr_acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbNSEb_TrdYg",
        "outputId": "57931677-6da3-47e4-b1ac-371fd0c922d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accuracy is 94.7368 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Set probability=True to access probas.\n",
        "svm_model = SVC(probability=True, random_state=1)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_acc = accuracy_score(y_test, svm_preds)\n",
        "print('SVM accuracy is {0:7.4f} %'.format(svm_acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APLu1dXJrdYg",
        "outputId": "46df6d66-f4e5-4f99-a01f-8303a518973e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree accuracy is 93.8596 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_model = DecisionTreeClassifier(random_state=1)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_preds = tree_model.predict(X_test)\n",
        "\n",
        "tree_acc = accuracy_score(y_test, tree_preds)\n",
        "print('Decision Tree accuracy is {0:7.4f} %'.format(tree_acc * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's build a hard and soft voting mechanism."
      ],
      "metadata": {
        "id": "-8zhJwThiVij"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBuolfMdrdYg",
        "outputId": "a6d04d6c-b20f-4145-b651-92d7b47bc1b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Voting Classifier accuracy: 96.4912%\n",
            "Soft Voting Classifier accuracy: 95.6140%\n"
          ]
        }
      ],
      "source": [
        "# hard voting (majority vote)\n",
        "predictions = np.array([lr_preds, svm_preds, tree_preds])\n",
        "ensemble_hard_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
        "\n",
        "# soft voting (average probability)\n",
        "log_reg_probs = lr_model.predict_proba(X_test)\n",
        "svc_probs = svm_model.predict_proba(X_test)\n",
        "tree_probs = tree_model.predict_proba(X_test)\n",
        "avg_probs = (log_reg_probs + svc_probs + tree_probs) / 3\n",
        "ensemble_soft_preds = np.argmax(avg_probs, axis=1)\n",
        "\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Hard Voting Classifier accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Soft Voting Classifier accuracy: {soft_acc * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can validate using Scikit learn [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)."
      ],
      "metadata": {
        "id": "DHlu7n5QhoOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "hard_voting_clf = VotingClassifier(estimators=[('lr', lr_model), ('svc', svm_model), ('tree', tree_model)], voting='hard')\n",
        "hard_voting_clf.fit(X_train, y_train)\n",
        "ensemble_hard_preds = hard_voting_clf.predict(X_test)\n",
        "\n",
        "soft_voting_clf = VotingClassifier(estimators=[('lr', lr_model), ('svc', svm_model), ('tree', tree_model)], voting='soft')\n",
        "soft_voting_clf.fit(X_train, y_train)\n",
        "ensemble_soft_preds = soft_voting_clf.predict(X_test)\n",
        "\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Hard Voting Classifier accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Soft Voting Classifier accuracy: {soft_acc * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJjSg-3nj2iv",
        "outputId": "ebddecb3-7e97-4859-f22e-862a8df22375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Voting Classifier accuracy: 96.4912%\n",
            "Soft Voting Classifier accuracy: 95.6140%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqrDCNoMrdYk"
      },
      "source": [
        "## Bagging Ensemble\n",
        "\n",
        "The term **bagging** is short for *bootstrap aggregating*, where bootstrapping refers to sampling subsets from the training data with replacement. Bagging is an ensemble learning technique that aims to reduce variance and improve model stability by training multiple base learners on different random subsets of the dataset and aggregating their predictions.\n",
        "\n",
        "Given a training dataset of size $N$, bagging creates $m$ bootstrap samples $D_1, D_2, ..., D_m$ by randomly sampling with replacement from the original dataset.\n",
        "\n",
        "Each base model $f_j(x)$ is trained independently on a bootstrap sample $D_j$ and produces a prediction $\\hat{y}_i^j$ for an input sample $x_i$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1V3rJ-brdYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "n_estimators = 10\n",
        "tree_models = []\n",
        "for _ in range(n_estimators):\n",
        "    X_resampled, y_resampled = resample(X_train, y_train, random_state=1)\n",
        "    tree = DecisionTreeClassifier(random_state=1)\n",
        "    tree.fit(X_resampled, y_resampled)\n",
        "    tree_models.append(tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wsDTsx-rdYk",
        "outputId": "1247378f-eebb-4941-8fa7-2aadd4514ecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Hard Voting Classifier accuracy: 92.9825%\n",
            "Bagging Soft Voting Classifier accuracy: 92.9825%\n"
          ]
        }
      ],
      "source": [
        "tree_preds = np.array([tree.predict(X_test) for tree in tree_models])\n",
        "ensemble_hard_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_preds)\n",
        "\n",
        "tree_probs = np.mean([tree.predict_proba(X_test) for tree in tree_models], axis=0)\n",
        "ensemble_soft_preds = np.argmax(tree_probs, axis=1)\n",
        "\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Bagging Hard Voting Classifier accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Bagging Soft Voting Classifier accuracy: {soft_acc * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG1iglaQrdYl"
      },
      "source": [
        "Bagging Ensembles often outperform Voting Ensembles, and they can decrease the overfitting of Decision Trees (which tend to overfit easily). As well as, Bagging Ensembles can be trained in parallel using different CPU cores, which can reduce the processing time.  \n",
        "\n",
        "Random Forest can be considered a Bagging Ensemble trained on the whole dataset if you set max samples to 1, in fact let's check using scikit [RandomForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=10, max_samples=1.0, bootstrap=True, random_state=1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "ensemble_hard_preds = rf_model.predict(X_test)\n",
        "ensemble_soft_preds = np.argmax(rf_model.predict_proba(X_test), axis=1)\n",
        "hard_acc = accuracy_score(y_test, ensemble_hard_preds)\n",
        "soft_acc = accuracy_score(y_test, ensemble_soft_preds)\n",
        "print(f'Random Forest (Bagging) Hard Voting accuracy: {hard_acc * 100:.4f}%')\n",
        "print(f'Random Forest (Bagging) Soft Voting accuracy: {soft_acc * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4SUPcqDmC7S",
        "outputId": "c113be7f-0d7a-4586-d217-57465aa62982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest (Bagging) Hard Voting accuracy: 95.6140%\n",
            "Random Forest (Bagging) Soft Voting accuracy: 95.6140%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrc0HpHNrdYl"
      },
      "source": [
        "## Boosting Ensemble\n",
        "\n",
        "Boosting is an ensemble learning technique that trains individual models **sequentially**, where each subsequent model learns from the errors of the previous models and attempts to improve overall performance. Unlike bagging, which trains models independently in parallel, boosting adjusts model weights dynamically to focus on **misclassified samples**.\n",
        "\n",
        "Given a dataset with $N$ training samples, boosting maintains a weight distribution $w_i$ for each training sample, adjusting it iteratively to give more importance to **misclassified samples**.\n",
        "\n",
        "Each model $f_t(x)$ is trained sequentially on a weighted version of the dataset, where misclassified instances receive higher weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK52Ix40rdYl"
      },
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "Gradient Boosting is an ensemble learning method that sequentially trains weak models, typically decision trees, to correct errors made by the previous models. It leverages gradient descent to minimize a predefined loss function, making it effective for both regression and classification.\n",
        "\n",
        "The initial model $ F_0(x) $ is chosen to minimize the loss function over the training data. For regression we have:\n",
        "\n",
        "$$\n",
        "F_0(x) = \\frac{1}{N} \\sum_{i=1}^{N} y_i\n",
        "$$\n",
        "\n",
        "For classification (e.g., with log loss), the initial prediction is based on the log-odds:\n",
        "\n",
        "$$\n",
        "F_0(x) = \\log \\frac{p}{1 - p}\n",
        "$$\n",
        "\n",
        "where $ p $ is the prior probability of the positive class.\n",
        "\n",
        "\n",
        "For each boosting step $ m $, compute the **pseudo-residuals**, which represent the negative gradient of the loss function with respect to the current model's predictions. For squared error loss $ L(y_i, F) = (y_i - F)^2 $, this simplifies to:\n",
        "\n",
        "$$\n",
        "r_i^{(m)} = y_i - F_{m-1}(x_i)\n",
        "$$\n",
        "\n",
        "For log loss in binary classification:\n",
        "\n",
        "$$\n",
        "r_i^{(m)} = y_i - \\sigma(F_{m-1}(x_i))\n",
        "$$\n",
        "\n",
        "where $ \\sigma(F) = \\frac{1}{1 + e^{-F}} $ is the sigmoid function.\n",
        "\n",
        "Train a weak model (e.g., a decision tree) $ h_m(x) $ to predict the residuals:\n",
        "\n",
        "$$\n",
        "h_m(x) = \\arg\\min_h \\sum_{i=1}^{N} \\left( r_i^{(m)} - h(x_i) \\right)^2\n",
        "$$\n",
        "\n",
        "This weak learner captures patterns in the residuals. To determine the optimal step size $ \\rho_m $, minimize the loss function:\n",
        "\n",
        "$$\n",
        "\\rho_m = \\arg\\min_\\rho \\sum_{i=1}^{N} L(y_i, F_{m-1}(x_i) + \\rho h_m(x_i))\n",
        "$$\n",
        "\n",
        "For squared error loss:\n",
        "\n",
        "$$\n",
        "\\rho_m = \\frac{\\sum_{i=1}^{N} r_i^{(m)} h_m(x_i)}{\\sum_{i=1}^{N} h_m(x_i)^2}\n",
        "$$\n",
        "\n",
        "\n",
        "The new model is updated by adding the scaled weak learner:\n",
        "\n",
        "$$\n",
        "F_m(x) = F_{m-1}(x) + \\eta \\rho_m h_m(x)\n",
        "$$\n",
        "\n",
        "where $ \\eta $ is the learning rate, controlling the contribution of each weak learner.\n",
        "\n",
        "obtaining the final model:\n",
        "\n",
        "$$\n",
        "F(x) = F_0(x) + \\sum_{m=1}^{M} \\eta \\rho_m h_m(x)\n",
        "$$\n",
        "\n",
        "For regression, $ F(x) $ is the predicted value. For classification, predictions are obtained via:\n",
        "\n",
        "$$\n",
        "P(y = 1 \\mid x) = \\sigma(F(x))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "\n",
        "n_estimators = 50\n",
        "learning_rate = 0.1\n",
        "max_depth = 3\n",
        "p = np.mean(y_train)\n",
        "F_train = np.full_like(y_train, np.log(p / (1 - p)), dtype=np.float64)\n",
        "\n",
        "for _ in range(n_estimators):\n",
        "    # Compute pseudo residuals\n",
        "    p_train = 1 / (1 + np.exp(-F_train))\n",
        "    residuals = y_train - p_train\n",
        "\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=5)\n",
        "    tree.fit(X_train, residuals)\n",
        "\n",
        "    tree_preds = tree.predict(X_train)\n",
        "\n",
        "    numerator = np.sum(residuals * tree_preds)\n",
        "    denominator = np.sum(tree_preds * tree_preds)\n",
        "    rho = numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "    F_train += learning_rate * rho * tree_preds\n",
        "\n",
        "F_test = np.full_like(y_test, np.log(p / (1 - p)), dtype=np.float64)\n",
        "for _ in range(n_estimators):\n",
        "    tree_preds_test = tree.predict(X_test)\n",
        "    F_test += learning_rate * rho * tree_preds_test\n",
        "ensemble_probs = 1 / (1 + np.exp(-F_test))\n",
        "ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
        "\n",
        "logloss = log_loss(y_test, ensemble_probs)\n",
        "accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f'Gradient Boosting Log Loss: {logloss:.4f}')\n",
        "print(f'Gradient Boosting Accuracy: {accuracy * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H59fP1Iqn6ad",
        "outputId": "8dc2a76c-baec-4219-f4ee-542f6ef2fd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Log Loss: 0.3612\n",
            "Gradient Boosting Accuracy: 96.4912%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it against Scikit's [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)"
      ],
      "metadata": {
        "id": "QUvexYXaoQQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovarxTzirdYl",
        "outputId": "f1d45389-ea30-4ed1-ac58-9a191a284d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Log Loss: 0.1121\n",
            "Gradient Boosting Accuracy: 95.6140%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "boosting_model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=1)\n",
        "boosting_model.fit(X_train, y_train)\n",
        "\n",
        "ensemble_probs = boosting_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "ensemble_preds = boosting_model.predict(X_test)\n",
        "logloss = log_loss(y_test, ensemble_probs)\n",
        "accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f'Gradient Boosting Log Loss: {logloss:.4f}')\n",
        "print(f'Gradient Boosting Accuracy: {accuracy * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZacirDzfrdYn"
      },
      "source": [
        "# Stacking Methods\n",
        "\n",
        "Stacking is an ensemble learning technique that combines multiple base models by training a higher-level **meta-model** to aggregate their predictions. Unlike hard or soft voting methods, stacking learns how to best combine base model outputs by leveraging another model that specializes in integrating predictions.\n",
        "\n",
        "Let $f_1(x), f_2(x), ..., f_m(x)$ represent the $m$ base models. For a given input $x$, the predictions from the base models are:\n",
        "\n",
        "$$\n",
        "\\mathbf{h}(x) = [f_1(x), f_2(x), ..., f_m(x)]\n",
        "$$\n",
        "\n",
        "These predictions form a new feature vector $\\mathbf{h}(x)$ for the meta-model $g(\\mathbf{h}(x))$. The final stacked prediction is then:\n",
        "\n",
        "$$\n",
        "\\hat{y} = g(\\mathbf{h}(x))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg = LogisticRegression(max_iter=10000, random_state=1)\n",
        "log_reg.fit(X_train, y_train)\n",
        "log_reg_preds = log_reg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "svc_model = SVC(probability=True, random_state=1)\n",
        "svc_model.fit(X_train, y_train)\n",
        "svc_preds = svc_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "tree_model = DecisionTreeClassifier(random_state=1)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_preds = tree_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "stacked_features = np.column_stack((log_reg_preds, svc_preds, tree_preds))\n",
        "\n",
        "meta_model = LogisticRegression(max_iter=10000, random_state=1)\n",
        "meta_model.fit(stacked_features, y_test)\n",
        "meta_preds = meta_model.predict_proba(stacked_features)[:, 1]\n",
        "final_preds = meta_model.predict(stacked_features)\n",
        "\n",
        "logloss = log_loss(y_test, meta_preds)\n",
        "accuracy = accuracy_score(y_test, final_preds)\n",
        "print(f'Stacking Log Loss: {logloss:.4f}')\n",
        "print(f'Stacking Accuracy: {accuracy * 100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuyzBibrrYt4",
        "outputId": "9a747a7c-90d4-4f6f-c519-afb1237dcf1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Log Loss: 0.1162\n",
            "Stacking Accuracy: 96.4912%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-NPPn8VrdYn",
        "outputId": "1c5d4e6f-541c-41be-cf1a-f8d4d61ea9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Log Loss: 0.0985\n",
            "Stacking Accuracy: 95.6140%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('log_reg', LogisticRegression(max_iter=10000, random_state=1)),\n",
        "    ('svc', SVC(probability=True, random_state=1)),\n",
        "    ('tree', DecisionTreeClassifier(random_state=1))\n",
        "]\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=10000, random_state=1))\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "ensemble_probs = stacking_model.predict_proba(X_test)[:, 1]  # Probability of class 1\n",
        "ensemble_preds = stacking_model.predict(X_test)\n",
        "logloss = log_loss(y_test, ensemble_probs)\n",
        "accuracy = accuracy_score(y_test, ensemble_preds)\n",
        "print(f'Stacking Log Loss: {logloss:.4f}')\n",
        "print(f'Stacking Accuracy: {accuracy * 100:.4f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLvFnn_ArdYn"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we have explored several key ensemble learning techniques and their unique strengths:\n",
        "\n",
        "1. **Bagging:** By training multiple models on different bootstrapped subsets of the data, we learned how to reduce variance and create a robust ensemble that outperforms individual models. The Random Forest is a classic example that demonstrates bagging’s power to handle noise and provide stable predictions.\n",
        "\n",
        "2. **Boosting:** We saw how boosting sequentially trains weak learners, focusing on correcting the errors of prior models. By optimizing the loss function step-by-step, boosting algorithms—such as AdaBoost and Gradient Boosting—achieve high accuracy and are especially effective at handling complex patterns and imbalanced datasets.\n",
        "\n",
        "3. **Voting:** Hard and soft voting methods taught us how simple aggregations of multiple models’ predictions can yield improved performance. These approaches illustrate that even without sophisticated training strategies, combining diverse model types can enhance overall accuracy.\n",
        "\n",
        "4. **Stacking:** Finally, stacking introduced the concept of training a meta-model to learn how best to combine the outputs of multiple base models. By using a higher-level model to integrate predictions, we gain the ability to exploit the complementary strengths of various learning algorithms.\n",
        "\n",
        "We have seen that ensemble methods are some of the most effective tools in the machine learning toolbox. They consistently deliver top-tier performance on tabular data, often with minimal hyperparameter tuning. By understanding and leveraging these techniques, we can tackle a wide range of predictive tasks with confidence and reliability.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}